{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asaad972/CollabFirstNoteBook/blob/main/HW02_Cloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcwcS_-VsXcf"
      },
      "outputs": [],
      "source": [
        "# CELL 1: Minimal package installation (only if missing)\n",
        "import importlib.util, sys, subprocess\n",
        "\n",
        "def ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "# Usually already installed in Colab, but keep safe:\n",
        "ensure(\"pandas\", \"pandas\")\n",
        "\n",
        "# Check if installiation is not done\n",
        "ensure(\"nltk\", \"nltk\")\n",
        "ensure(\"sentence-transformers\", \"sentence_transformers\")\n",
        "ensure(\"faiss-cpu\", \"faiss\")\n",
        "ensure(\"pymupdf\", \"fitz\")\n",
        "\n",
        "\n",
        "print(\" Dependencies ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Imports + NLTK resources (run once per runtime)\n",
        "\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# NLTK downloads (required for stopwords/tokenizer/lemmatizer)\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "print(\" Imports ready + NLTK resources downloaded\")\n"
      ],
      "metadata": {
        "id": "2wDldDHHb8tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Store Classes (Vector Store + Inverted Index)\n",
        "# =====================================================\n",
        "\"\"\"\n",
        " CELL 3: STORE CLASSES\n",
        "- SimpleVectorStore: stores embeddings + documents + metadatas + ids (like Tirgul 7)\n",
        "- InvertedIndexStore: stores required index schema term -> DocIDs (homework requirement)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- Vector Store (similar to Tirgul 7) ----------\n",
        "class SimpleVectorStore:\n",
        "    \"\"\"Simple in-memory vector store (fallback)\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.documents = []\n",
        "        self.embeddings = []   # list of numpy arrays\n",
        "        self.metadatas = []\n",
        "        self.ids = []\n",
        "        print(\" SimpleVectorStore initialized\")\n",
        "\n",
        "    def add(self, embeddings, documents, metadatas, ids):\n",
        "        # Ensure numpy arrays\n",
        "        embeddings = [np.asarray(e, dtype=np.float32) for e in embeddings]\n",
        "        self.embeddings.extend(embeddings)\n",
        "        self.documents.extend(documents)\n",
        "        self.metadatas.extend(metadatas)\n",
        "        self.ids.extend(ids)\n",
        "        print(f\" Added {len(documents)} documents to simple vector store\")\n",
        "\n",
        "    def query(self, query_embeddings, n_results=5):\n",
        "        if not self.embeddings:\n",
        "            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}\n",
        "\n",
        "        q = np.asarray(query_embeddings[0], dtype=np.float32)\n",
        "\n",
        "        E = np.vstack(self.embeddings)  # shape: (N, d)\n",
        "\n",
        "        # cosine similarity without sklearn\n",
        "        q_norm = np.linalg.norm(q) + 1e-12\n",
        "        E_norm = np.linalg.norm(E, axis=1) + 1e-12\n",
        "        sims = (E @ q) / (E_norm * q_norm)\n",
        "\n",
        "        top_idx = np.argsort(sims)[::-1][:n_results]\n",
        "\n",
        "        return {\n",
        "            'ids': [[self.ids[i] for i in top_idx]],\n",
        "            'documents': [[self.documents[i] for i in top_idx]],\n",
        "            'metadatas': [[self.metadatas[i] for i in top_idx]],\n",
        "            'distances': [[float(1 - sims[i]) for i in top_idx]]  # distance-like\n",
        "        }\n",
        "\n",
        "    def count(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "\n",
        "# ---------- Inverted Index (required by homework) ----------\n",
        "class InvertedIndexStore:\n",
        "    \"\"\"Required structure: term -> DocIDs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.term_to_docids = defaultdict(set)\n",
        "        print(\" InvertedIndexStore initialized\")\n",
        "\n",
        "    def add_occurrence(self, term: str, doc_id: str):\n",
        "        self.term_to_docids[term].add(doc_id)\n",
        "\n",
        "    def get_docids(self, term: str):\n",
        "        return sorted(self.term_to_docids.get(term, set()))\n",
        "\n",
        "    def count_terms(self) -> int:\n",
        "        return len(self.term_to_docids)\n",
        "\n",
        "    def to_required_format(self):\n",
        "        # [{\"term\": ..., \"DocIDs\": [...]}, ...]\n",
        "        return [{\"term\": t, \"DocIDs\": sorted(list(docids))}\n",
        "                for t, docids in sorted(self.term_to_docids.items())]\n",
        "\n",
        "\n",
        "print(\" Store classes defined!\")\n",
        "print(\" Next: Cell 4 (core logic: preprocess + build index + embeddings)\")\n"
      ],
      "metadata": {
        "id": "H6h6ichmcgHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Core setup (custom stopwords + stemming + embedding model + FAISS)\n",
        "\n",
        "# --- Custom stopwords (you define them) ---\n",
        "# We remove these words because they are very frequent function words (articles, prepositions, pronouns).\n",
        "# They usually do not add topic meaning, but they increase index size and add noise to retrieval.\n",
        "CUSTOM_STOPWORDS = {\n",
        "    \"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\n",
        "    \"to\",\"of\",\"in\",\"on\",\"at\",\"for\",\"from\",\"by\",\"with\",\"as\",\n",
        "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"this\",\"that\",\"these\",\"those\",\n",
        "    \"it\",\"its\",\"they\",\"them\",\"their\",\"we\",\"our\",\"you\",\"your\",\n",
        "    \"i\",\"me\",\"my\",\"he\",\"him\",\"his\",\"she\",\"her\",\n",
        "    \"not\",\"no\",\"do\",\"does\",\"did\",\"doing\"\n",
        "}\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    \"\"\"\n",
        "    Returns list of terms for indexing:\n",
        "    - lowercase\n",
        "    - tokenize\n",
        "    - keep alphabetic tokens only\n",
        "    - remove custom stopwords\n",
        "    - apply stemming\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    terms = []\n",
        "    for tok in tokens:\n",
        "        if tok.isalpha() and tok not in CUSTOM_STOPWORDS:\n",
        "            terms.append(stemmer.stem(tok))\n",
        "    return terms\n",
        "\n",
        "# --- Embedding model (for semantic retrieval) ---\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# --- FAISS index (stores embeddings for doc-level retrieval) ---\n",
        "faiss_index = None\n",
        "vector_dim = None\n",
        "\n",
        "# Parallel stores (FAISS row -> doc data)\n",
        "vector_doc_ids = []   # doc_id\n",
        "vector_texts = []     # full doc text\n",
        "\n",
        "print(\" Core setup ready (custom stopwords + stemming + embeddings + FAISS)\")"
      ],
      "metadata": {
        "id": "eOcug8jcPQ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NEW CELL: GEMINI SETUP ---\n",
        "!pip install -q -U google-generativeai\n",
        "import google.generativeai as genai\n",
        "\n",
        "# REPLACE WITH YOUR KEY\n",
        "genai.configure(api_key=\"AIzaSyBN7qfLTK2hcgctWkIUrnZc4efJMZB0n9I\")\n",
        "\n",
        "def ask_gemini(context, user_question):\n",
        "    if not context: return \"No relevant info found.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Answer based ONLY on this context:\n",
        "    {context}\n",
        "\n",
        "    Question: {user_question}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-pro')\n",
        "        return model.generate_content(prompt).text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "print(\"‚úÖ Gemini AI is ready!\")"
      ],
      "metadata": {
        "id": "3Dg4VvuVp9vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Wikipedia source links (seed documents for the corpus)\n",
        "\n",
        "wiki_links = [\n",
        "    \"https://en.wikipedia.org/wiki/Plant_disease\",\n",
        "    \"https://en.wikipedia.org/wiki/Plant_pathology\",\n",
        "    \"https://en.wikipedia.org/wiki/Fungus\",\n",
        "    \"https://en.wikipedia.org/wiki/Bacterial_wilt\",\n",
        "    \"https://en.wikipedia.org/wiki/Powdery_mildew\"\n",
        "]\n",
        "\n",
        "print(\"Wikipedia links used:\")\n",
        "for i, link in enumerate(wiki_links, 1):\n",
        "    print(f\"{i}. {link}\")\n"
      ],
      "metadata": {
        "id": "F5Swqy3GdK37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Load documents from Wikipedia (API fetch + normalization + metadata)\n",
        "\n",
        "import requests\n",
        "import re\n",
        "\n",
        "WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "# Wikipedia blocks requests without a proper User-Agent sometimes\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"HW02-Cloud-RAG/1.0 (student project; contact: student@example.com)\"\n",
        "}\n",
        "\n",
        "def title_from_wiki_url(url: str) -> str:\n",
        "    if \"/wiki/\" not in url:\n",
        "        raise ValueError(f\"Unsupported Wikipedia URL: {url}\")\n",
        "    title = url.split(\"/wiki/\", 1)[1]\n",
        "    title = title.split(\"#\", 1)[0]      # remove anchors\n",
        "    title = title.replace(\"_\", \" \")\n",
        "    return title\n",
        "\n",
        "def fetch_page_extract_by_title(title: str):\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"prop\": \"extracts|info\",\n",
        "        \"titles\": title,\n",
        "        \"inprop\": \"url\",\n",
        "        \"explaintext\": True,\n",
        "        \"redirects\": 1,   # follow redirects\n",
        "        \"origin\": \"*\"     # helps in some environments\n",
        "    }\n",
        "    r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=30)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    pages = r.json()[\"query\"][\"pages\"]\n",
        "    page = next(iter(pages.values()))\n",
        "\n",
        "    # Handle missing page\n",
        "    if \"missing\" in page:\n",
        "        return {\"pageid\": None, \"title\": title, \"url\": \"\", \"text\": \"\"}\n",
        "\n",
        "    return {\n",
        "        \"pageid\": page.get(\"pageid\"),\n",
        "        \"title\": page.get(\"title\", title),\n",
        "        \"url\": page.get(\"fullurl\", \"\"),\n",
        "        \"text\": page.get(\"extract\", \"\")\n",
        "    }\n",
        "\n",
        "def slugify(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s)\n",
        "    return s.strip(\"-\")\n",
        "\n",
        "def load_docs_from_wiki_links(wiki_links):\n",
        "    docs = {}\n",
        "    docs_meta = {}\n",
        "\n",
        "    for url in wiki_links:\n",
        "        title = title_from_wiki_url(url)\n",
        "        data = fetch_page_extract_by_title(title)\n",
        "\n",
        "        text = (data.get(\"text\") or \"\").strip()\n",
        "        if not text:\n",
        "            print(f\"Empty/blocked page: {title} | {url}\")\n",
        "            continue\n",
        "\n",
        "        doc_id = f\"wiki_{slugify(data['title'])}\"\n",
        "        docs[doc_id] = text\n",
        "        docs_meta[doc_id] = {\n",
        "            \"title\": data[\"title\"],\n",
        "            \"url\": data.get(\"url\") or url,\n",
        "            \"source\": \"wikipedia\",\n",
        "            \"pageid\": data.get(\"pageid\"),\n",
        "        }\n",
        "\n",
        "        print(f\"Loaded: {data['title']} -> {doc_id} | chars={len(text)}\")\n",
        "\n",
        "    return docs, docs_meta\n",
        "\n",
        "docs, docs_meta = load_docs_from_wiki_links(wiki_links)\n",
        "print(\"Docs loaded:\", len(docs))\n"
      ],
      "metadata": {
        "id": "ka2aEOAOgS2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Build the required index (term -> DocIDs) + build FAISS embeddings store (doc-level)\n",
        "\n",
        "# 1) Build inverted index (term -> DocIDs)\n",
        "inv_index = InvertedIndexStore()\n",
        "\n",
        "for doc_id, text in docs.items():\n",
        "    terms = preprocess_text(text)   # uses custom stopwords + stemming\n",
        "    for t in set(terms):            # presence only (not frequency)\n",
        "        inv_index.add_occurrence(t, doc_id)\n",
        "\n",
        "print(f\" Inverted index built. Unique terms: {inv_index.count_terms()}\")\n",
        "\n",
        "# 2) Build embeddings + FAISS (one vector per doc)\n",
        "doc_ids = list(docs.keys())\n",
        "texts = [docs[d] for d in doc_ids]\n",
        "\n",
        "emb = embed_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "\n",
        "vector_dim = emb.shape[1]\n",
        "faiss_index = faiss.IndexFlatIP(vector_dim)  # cosine similarity via normalized embeddings\n",
        "faiss_index.add(emb)\n",
        "\n",
        "# parallel arrays for retrieval results\n",
        "vector_doc_ids = doc_ids\n",
        "vector_texts = texts\n",
        "\n",
        "print(f\" FAISS built. Vectors: {faiss_index.ntotal} | dim={vector_dim}\")\n"
      ],
      "metadata": {
        "id": "x1SNUhlfQPfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Firebase Initialization (Hybrid Safe Mode)\n",
        "\n",
        "!pip -q install firebase-admin\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "import base64\n",
        "\n",
        "# --- 1. Public Configuration (Safe to share) ---\n",
        "# We keep the standard info visible so the code is easy to understand.\n",
        "config = {\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"hw02-cloud-inverted-index\",\n",
        "  \"private_key_id\": \"437db7abaab45e69cf2bf0c22aa8c2e23cbbc71e\",\n",
        "  \"client_email\": \"firebase-adminsdk-fbsvc@hw02-cloud-inverted-index.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"105185385505390955098\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/firebase-adminsdk-fbsvc%40hw02-cloud-inverted-index.iam.gserviceaccount.com\",\n",
        "  \"universe_domain\": \"googleapis.com\"\n",
        "}\n",
        "\n",
        "# --- 2. Private Key (Hidden) ---\n",
        "# Paste the string you generated in Step 1 here.\n",
        "scrambled_key = \"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\"\n",
        "\n",
        "try:\n",
        "    # We unlock the key and add it to the config\n",
        "    config[\"private_key\"] = base64.b64decode(scrambled_key).decode('utf-8')\n",
        "\n",
        "    # Initialize Firebase\n",
        "    cred = credentials.Certificate(config)\n",
        "\n",
        "    if not firebase_admin._apps:\n",
        "        firebase_admin.initialize_app(cred)\n",
        "\n",
        "    db = firestore.client()\n",
        "    print(\"Firestore connected successfully to:\", db.project)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "z3PfU1boz_l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Upload inverted index to Firestore (cloud storage of term -> DocIDs)\n",
        "\n",
        "# Fix 1: Removed unused 'ArrayUnion' import\n",
        "# Fix 2: Added 'db' as an argument to link explicitly to Cell 8\n",
        "\n",
        "def upload_inverted_index(inv_index, db_client, collection_name=\"inverted_index\", batch_size=400):\n",
        "    \"\"\"\n",
        "    Uploads the inverted index to Firestore.\n",
        "    \"\"\"\n",
        "    col = db_client.collection(collection_name)\n",
        "\n",
        "    # Ensure your inv_index object has this method.\n",
        "    # If inv_index is just a plain dict, use: records = [{\"term\": t, \"DocIDs\": d} for t, d in inv_index.items()]\n",
        "    records = inv_index.to_required_format()\n",
        "\n",
        "    batch = db_client.batch()\n",
        "    ops = 0\n",
        "\n",
        "    for r in records:\n",
        "        term = r[\"term\"]\n",
        "        doc_ids = r[\"DocIDs\"]\n",
        "\n",
        "        # Fix 3: SANITIZE THE ID.\n",
        "        # Firestore IDs cannot contain '/'. We replace it with '_' or simple URL encoding.\n",
        "        safe_term = term.replace(\"/\", \"_\")\n",
        "\n",
        "        # Limit length to 1500 bytes (Firestore limit per ID)\n",
        "        doc_id = safe_term[:1500]\n",
        "\n",
        "        ref = col.document(doc_id)\n",
        "        batch.set(ref, {\n",
        "            \"term\": term,         # Store original term inside the document\n",
        "            \"doc_ids\": doc_ids,\n",
        "            \"df\": len(doc_ids),\n",
        "        })\n",
        "\n",
        "        ops += 1\n",
        "        if ops >= batch_size:\n",
        "            batch.commit()\n",
        "            batch = db_client.batch()\n",
        "            ops = 0\n",
        "\n",
        "    if ops > 0:\n",
        "        batch.commit()\n",
        "\n",
        "    print(f\"Uploaded {len(records)} terms to Firestore collection '{collection_name}'\")\n",
        "\n",
        "# Execute the upload passing the 'db' from Cell 8\n",
        "upload_inverted_index(inv_index, db)"
      ],
      "metadata": {
        "id": "3GMKBFBw1sdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Upload Wikipedia document metadata to Firestore (documents collection)\n",
        "\n",
        "def upload_wiki_meta(docs_meta, collection_name=\"documents\", batch_size=400):\n",
        "    \"\"\"\n",
        "    Uploads Wikipedia document metadata to Firestore.\n",
        "\n",
        "    Each document is stored as:\n",
        "      documents/{doc_id}\n",
        "\n",
        "    Stored fields:\n",
        "      - doc_id  : your internal document ID (e.g., wiki_plant-disease)\n",
        "      - title   : Wikipedia page title\n",
        "      - url     : Wikipedia page URL\n",
        "      - source  : \"wikipedia\"\n",
        "      - pageid  : Wikipedia page id (if available)\n",
        "\n",
        "    This does NOT upload the full article text; it only uploads metadata.\n",
        "    \"\"\"\n",
        "    col = db.collection(collection_name)\n",
        "\n",
        "    batch = db.batch()\n",
        "    ops = 0\n",
        "\n",
        "    for doc_id, meta in docs_meta.items():\n",
        "        ref = col.document(doc_id)\n",
        "        batch.set(ref, {\n",
        "            \"doc_id\": doc_id,\n",
        "            \"title\": meta.get(\"title\", \"\"),\n",
        "            \"url\": meta.get(\"url\", \"\"),\n",
        "            \"source\": meta.get(\"source\", \"wikipedia\"),\n",
        "            \"pageid\": meta.get(\"pageid\", None),\n",
        "        }, merge=True)\n",
        "\n",
        "        ops += 1\n",
        "        if ops >= batch_size:\n",
        "            batch.commit()\n",
        "            batch = db.batch()\n",
        "            ops = 0\n",
        "\n",
        "    if ops > 0:\n",
        "        batch.commit()\n",
        "\n",
        "    print(f\"Uploaded {len(docs_meta)} wiki docs to '{collection_name}'\")\n",
        "\n",
        "# Upload metadata for the loaded Wikipedia docs\n",
        "upload_wiki_meta(docs_meta)\n"
      ],
      "metadata": {
        "id": "Al003f-n5Ubq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Embedding-based document retrieval using FAISS (semantic search)\n",
        "\n",
        "def retrieve_top_docs(query: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Retrieves the top-K most relevant documents for a user query using\n",
        "    vector embeddings and FAISS similarity search.\n",
        "\n",
        "    This function:\n",
        "      1) Embeds the query using the same embedding model as the documents\n",
        "      2) Searches the FAISS index using cosine similarity\n",
        "      3) Returns ranked documents with titles, similarity scores, and text snippets\n",
        "\n",
        "    Note: This is retrieval only (no generation / no LLM).\n",
        "    \"\"\"\n",
        "    if faiss_index is None or faiss_index.ntotal == 0:\n",
        "        return \"FAISS index is empty. Build vectors first.\"\n",
        "\n",
        "    # Embed and normalize the query\n",
        "    q_emb = embed_model.encode(\n",
        "        [query],\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    # Search FAISS index\n",
        "    distances, indices = faiss_index.search(q_emb, top_k)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"Query: {query}\")\n",
        "    lines.append(\"=\" * 60)\n",
        "\n",
        "    # Format ranked results\n",
        "    for rank, idx in enumerate(indices[0], start=1):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "\n",
        "        doc_id = vector_doc_ids[idx]\n",
        "        title = docs_meta.get(doc_id, {}).get(\"title\", \"\")\n",
        "        text = vector_texts[idx]\n",
        "        snippet = re.sub(r\"\\s+\", \" \", text)[:350]\n",
        "        score = float(distances[0][rank - 1])\n",
        "\n",
        "        lines.append(f\"{rank}) {doc_id} | {title} | similarity: {score:.4f}\")\n",
        "        lines.append(f\"Snippet: {snippet}...\")\n",
        "        lines.append(\"-\" * 60)\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "print(\"Retrieval function ready\")"
      ],
      "metadata": {
        "id": "dIvaQbcITaV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: RAG-style output (retrieval + \"enriched\" answer without OpenAI)\n",
        "# We will: retrieve top docs, then produce a simple enriched response by extracting key sentences.\n",
        "\n",
        "def split_sentences(text: str):\n",
        "    # simple sentence split (good enough for baseline)\n",
        "    parts = re.split(r'(?<=[.!?])\\s+', re.sub(r\"\\s+\", \" \", text).strip())\n",
        "    return [s for s in parts if len(s) > 30]\n",
        "\n",
        "def rag_answer_without_llm(query: str, top_k: int = 3, max_sentences_per_doc: int = 2):\n",
        "    if faiss_index is None or faiss_index.ntotal == 0:\n",
        "        return \"FAISS index is empty. Build vectors first.\"\n",
        "\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "    distances, indices = faiss_index.search(q_emb, top_k)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"Query: {query}\")\n",
        "    lines.append(\"=\" * 60)\n",
        "\n",
        "    # Retrieval section\n",
        "    lines.append(\"Top retrieved documents:\")\n",
        "    retrieved = []\n",
        "    for rank, idx in enumerate(indices[0], start=1):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        doc_id = vector_doc_ids[idx]\n",
        "        title = docs_meta.get(doc_id, {}).get(\"title\", \"\")\n",
        "        score = float(distances[0][rank - 1])\n",
        "        retrieved.append((doc_id, title, score))\n",
        "        lines.append(f\"{rank}) {doc_id} | {title} | similarity: {score:.4f}\")\n",
        "    lines.append(\"=\" * 60)\n",
        "\n",
        "    # Enriched response (extractive, no LLM)\n",
        "    lines.append(\"Enriched response (extractive, no LLM):\")\n",
        "    q_terms = set(preprocess_text(query))\n",
        "\n",
        "    for doc_id, title, score in retrieved:\n",
        "        text = docs[doc_id]\n",
        "        sents = split_sentences(text)\n",
        "\n",
        "        # score sentences by overlap with query terms (stems)\n",
        "        scored = []\n",
        "        for s in sents:\n",
        "            s_terms = set(preprocess_text(s))\n",
        "            overlap = len(q_terms & s_terms)\n",
        "            if overlap > 0:\n",
        "                scored.append((overlap, s))\n",
        "\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "        best = [s for _, s in scored[:max_sentences_per_doc]]\n",
        "\n",
        "        lines.append(f\"- Source: {doc_id} | {title}\")\n",
        "        if best:\n",
        "            for b in best:\n",
        "                lines.append(f\"  ‚Ä¢ {b}\")\n",
        "        else:\n",
        "            lines.append(\"  ‚Ä¢ (No strong matching sentences found)\")\n",
        "        lines.append(\"-\" * 60)\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "print(\" RAG-style (no OpenAI) function ready\")\n"
      ],
      "metadata": {
        "id": "yNqRTEkVPTuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Quick demo (edit the query text)\n",
        "\n",
        "print(retrieve_top_docs(\"how to detect plant diseases using sensors and ai\", top_k=3))\n",
        "print()\n",
        "print(rag_answer_without_llm(\"how to detect plant diseases using sensors and ai\", top_k=3))\n"
      ],
      "metadata": {
        "id": "UXh_buFURvu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# --- 1. SETUP THE WIDGETS (Standalone) ---\n",
        "ai_input = widgets.Text(\n",
        "    placeholder=\"Ask Gemini about your documents...\",\n",
        "    description=\"Question:\",\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "\n",
        "ai_btn = widgets.Button(\n",
        "    description=\"Ask AI\",\n",
        "    button_style='success', # Green\n",
        "    icon='star'\n",
        ")\n",
        "\n",
        "ai_output = widgets.Output()\n",
        "\n",
        "# --- 2. DEFINE THE LOGIC ---\n",
        "def on_ai_click(b):\n",
        "    ai_output.clear_output()\n",
        "    with ai_output:\n",
        "        q = ai_input.value\n",
        "        if not q:\n",
        "            print(\"Please enter a question.\")\n",
        "            return\n",
        "\n",
        "        print(f\"ü§ñ Gemini is thinking about: '{q}'...\")\n",
        "\n",
        "        # Check database\n",
        "        if faiss_index is None:\n",
        "             print(\"‚ùå Error: Database is empty. Please run the PDF loading cell first.\")\n",
        "             return\n",
        "\n",
        "        # A. RETRIEVE (Your RAG)\n",
        "        q_vec = embed_model.encode([q])\n",
        "        D, I = faiss_index.search(np.array(q_vec).astype('float32'), k=3)\n",
        "\n",
        "        found_texts = [vector_texts[i] for i in I[0] if i < len(vector_texts)]\n",
        "        context = \"\\n\".join(found_texts)\n",
        "\n",
        "        # B. GENERATE (Gemini)\n",
        "        if context:\n",
        "            answer = ask_gemini(context, q)\n",
        "            print(\"-\" * 60)\n",
        "            print(answer)\n",
        "            print(\"-\" * 60)\n",
        "        else:\n",
        "            print(\"No relevant info found in the documents.\")\n",
        "\n",
        "ai_btn.on_click(on_ai_click)\n",
        "\n",
        "# --- 3. DISPLAY IT ALONE ---\n",
        "# We create a simple box for this tool\n",
        "ai_tool = widgets.VBox([\n",
        "    widgets.HTML(\"<h2>üåø Independent AI Assistant</h2>\"),\n",
        "    widgets.HBox([ai_input, ai_btn]),\n",
        "    ai_output\n",
        "], layout=widgets.Layout(padding='20px', border='2px solid #4CAF50', margin='20px 0'))\n",
        "\n",
        "# Show it now!\n",
        "display(ai_tool)"
      ],
      "metadata": {
        "id": "RF7cSOs4uNvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FROM NOW ON. ASAAD'S PART"
      ],
      "metadata": {
        "id": "ZJL2UYRadFR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install firebase-admin ipywidgets matplotlib\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "from google.colab import userdata  # Import userdata\n",
        "import json\n",
        "\n",
        "# Check if Firebase is already running to avoid re-initialization error\n",
        "if not firebase_admin._apps:\n",
        "    # Use Colab Secrets instead of a file path\n",
        "    key_content = userdata.get('FIREBASE_KEY')\n",
        "    key_dict = json.loads(key_content)\n",
        "    cred = credentials.Certificate(key_dict)\n",
        "    firebase_admin.initialize_app(cred)\n",
        "\n",
        "# Get the client (works even if initialized in previous cells)\n",
        "db = firestore.client()\n",
        "# --- FIX END ---\n",
        "\n",
        "print(\"‚úÖ Connected to Firestore in project:\", db.project)"
      ],
      "metadata": {
        "id": "DmIJXYU1dIqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "BASE_URL = \"https://server-cloud-v645.onrender.com\"\n",
        "\n",
        "def fetch_history(feed: str, limit: int = 30) -> pd.DataFrame:\n",
        "    \"\"\"Fetch IoT history from course server. Returns DataFrame with created_at,value.\"\"\"\n",
        "    resp = requests.get(f\"{BASE_URL}/history\", params={\"feed\": feed, \"limit\": int(limit)}, timeout=120)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    if \"data\" not in data:\n",
        "        raise ValueError(f\"Server error: {data}\")\n",
        "\n",
        "    df = pd.DataFrame(data[\"data\"])\n",
        "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
        "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"created_at\", \"value\"]).sort_values(\"created_at\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "WfyJ7ggJnUBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip -q install transformers timm pillow torch --upgrade\n",
        "import io\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timezone\n",
        "from PIL import Image\n",
        "# hugging face import\n",
        "from transformers import pipeline\n",
        "# hugging face plant disease model id\n",
        "\n",
        "MODEL_ID = \"linkanjarad/mobilenet_v2_1.0_224-plant-disease-identification\"\n",
        "# load hugging face model here\n",
        "# clf will be used later to analyze the image\n",
        "clf = pipeline(\"image-classification\", model=MODEL_ID)\n",
        "# css for design only\n",
        "CSS = \"\"\"\n",
        "<style>\n",
        ":root {\n",
        "    --accent: #2563eb;\n",
        "    --accent-soft: #eff6ff;\n",
        "    --bg: #f8fafc;\n",
        "    --card-bg: #ffffff;\n",
        "    --text: #1e293b;\n",
        "}\n",
        "\n",
        ".jupyter-widgets, .widget-area {\n",
        "    font-family: 'Segoe UI', system-ui, sans-serif !important;\n",
        "}\n",
        "\n",
        "/* App wrapper */\n",
        ".app-shell {\n",
        "    background: var(--bg);\n",
        "    padding: 10px;\n",
        "    border-radius: 0 0 12px 12px;\n",
        "}\n",
        "\n",
        "/* Card */\n",
        ".modern-card {\n",
        "    background: var(--card-bg);\n",
        "    border-radius: 16px;\n",
        "    padding: 24px;\n",
        "    border: 1px solid #e2e8f0;\n",
        "    box-shadow: 0 4px 6px -1px rgba(15,23,42,0.1);\n",
        "    margin-bottom: 20px;\n",
        "}\n",
        ".modern-card h2 {\n",
        "    color: var(--accent);\n",
        "    margin: 0 0 8px 0;\n",
        "    font-size: 20px;\n",
        "    font-weight: 700;\n",
        "}\n",
        ".modern-card p {\n",
        "    color: #64748b;\n",
        "    font-size: 14px;\n",
        "    margin: 0 0 18px 0;\n",
        "}\n",
        "\n",
        "/* Tabs */\n",
        ".p-TabBar-tab {\n",
        "    background: transparent !important;\n",
        "    border: none !important;\n",
        "    color: #94a3b8 !important;\n",
        "    font-weight: 600 !important;\n",
        "    padding: 10px 20px !important;\n",
        "}\n",
        ".p-TabBar-tab.p-mod-current {\n",
        "    color: var(--accent) !important;\n",
        "    background: var(--accent-soft) !important;\n",
        "    border-radius: 10px !important;\n",
        "}\n",
        "\n",
        "/* Primary button */\n",
        ".btn-primary button {\n",
        "    background: var(--accent) !important;\n",
        "    color: white !important;\n",
        "    border-radius: 10px !important;\n",
        "    font-weight: 600 !important;\n",
        "    border: none !important;\n",
        "    padding: 8px 20px !important;\n",
        "    transition: transform 0.1s, box-shadow 0.1s;\n",
        "}\n",
        ".btn-primary button:active {\n",
        "    transform: scale(0.97);\n",
        "    box-shadow: 0 2px 5px rgba(15,23,42,0.15);\n",
        "}\n",
        "\n",
        "/* Warning button (Dashboard) */\n",
        ".btn-warning button {\n",
        "    background: #f97316 !important;\n",
        "    color: white !important;\n",
        "    border-radius: 10px !important;\n",
        "    font-weight: 600 !important;\n",
        "    border: none !important;\n",
        "    padding: 8px 20px !important;\n",
        "}\n",
        "\n",
        "/* FileUpload styled as upload zone */\n",
        ".widget-upload {\n",
        "    width: 100% !important;\n",
        "}\n",
        ".widget-upload > label {\n",
        "    width: 100%;\n",
        "    border: 2px dashed #cbd5e1;\n",
        "    border-radius: 16px;\n",
        "    background: #f1f5f9;\n",
        "    padding: 28px 16px;\n",
        "    text-align: center;\n",
        "    cursor: pointer;\n",
        "    font-weight: 600;\n",
        "    color: var(--accent);\n",
        "    font-size: 14px;\n",
        "    transition: all 0.2s ease;\n",
        "}\n",
        ".widget-upload > label:hover {\n",
        "    border-color: var(--accent);\n",
        "    background: var(--accent-soft);\n",
        "}\n",
        "\n",
        "/* Small hint text */\n",
        ".hint-text {\n",
        "    color: #64748b;\n",
        "    font-size: 11px;\n",
        "    margin-top: 4px;\n",
        "}\n",
        "\n",
        "/* Align labels */\n",
        ".labeled-field .widget-label {\n",
        "    min-width: 80px;\n",
        "}\n",
        "</style>\n",
        "\"\"\"\n",
        "# function to build cards for ui layout\n",
        "display(widgets.HTML(CSS))\n",
        "\n",
        "def create_card(title, subtitle, children):\n",
        "    header = widgets.HTML(f\"<h2>{title}</h2><p>{subtitle}</p>\")\n",
        "    box = widgets.VBox([header] + children)\n",
        "    box.add_class(\"modern-card\")\n",
        "    return box\n",
        "\n",
        "# SCREEN A ‚Äî PLANT DIAGNOSTIC\n",
        "a_out = widgets.Output()\n",
        "# input for plant name\n",
        "a_name = widgets.Text(\n",
        "    placeholder=\"Plant species (e.g. Tomato)\",\n",
        "    description=\"Plant:\",\n",
        "    style={'description_width': '70px'},\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "# widget to upload image\n",
        "\n",
        "a_uploader = widgets.FileUpload(\n",
        "    accept=\"image/*\",\n",
        "    multiple=False,\n",
        "    description=\"üì∑ Click to upload plant photo\"\n",
        ")\n",
        "a_uploader.layout = widgets.Layout(width='100%')\n",
        "a_hint = widgets.HTML(\"<div class='hint-text'>JPG/PNG ¬∑ One image at a time</div>\")\n",
        "\n",
        "a_btn = widgets.Button(\n",
        "    description=\"Run Analysis & Save\",\n",
        "    layout=widgets.Layout(width='100%', height='42px')\n",
        ")\n",
        "a_btn.add_class(\"btn-primary\")\n",
        "# when user uploads an image preview it\n",
        "def on_plant_upload(change):\n",
        "    with a_out:\n",
        "        clear_output()\n",
        "        if not a_uploader.value:\n",
        "            return\n",
        "        fname, f = list(a_uploader.value.items())[0]\n",
        "        display(widgets.Image(\n",
        "            value=f[\"content\"],\n",
        "            width=320,\n",
        "            layout=widgets.Layout(border='3px solid #e5efff', border_radius='12px')\n",
        "        ))\n",
        "        print(f\"\\nReady to analyze: {fname}\")\n",
        "\n",
        "a_uploader.observe(on_plant_upload, names=\"value\")\n",
        "# function that runs model when user clicks analyze\n",
        "\n",
        "def run_plant_analysis(_):\n",
        "    with a_out:\n",
        "        clear_output()\n",
        "         # check that image exists\n",
        "        if not a_uploader.value:\n",
        "            print(\" Please upload an image first.\")\n",
        "            return\n",
        "         # check name field\n",
        "        if not a_name.value.strip():\n",
        "            print(\" Please enter the plant name.\")\n",
        "            return\n",
        "\n",
        "        fname, f = list(a_uploader.value.items())[0]\n",
        "        img = Image.open(io.BytesIO(f[\"content\"])).convert(\"RGB\")\n",
        "\n",
        "        print(\"‚è≥ Analyzing plant health...\")\n",
        "        preds = clf(img)  #  hugging face model is used here we also take the best predict preds[0]\n",
        "        top = preds[0]\n",
        "\n",
        "        clear_output()\n",
        "        display(widgets.Image(value=f[\"content\"], width=320))\n",
        "        # detect color message green/red\n",
        "        healthy = \"healthy\" in top[\"label\"].lower()\n",
        "        bg = \"#16a34a\" if healthy else \"#dc2626\"\n",
        "\n",
        "        display(widgets.HTML(f\"\"\"\n",
        "            <div style=\"\n",
        "                background:{bg};\n",
        "                color:white;\n",
        "                padding:18px;\n",
        "                border-radius:12px;\n",
        "                margin-top:15px;\n",
        "                box-shadow:0 4px 10px rgba(0,0,0,0.15);\n",
        "            \">\n",
        "                <h3 style=\"margin:0; font-size:18px;\">\n",
        "                    Prediction: {top['label'].replace('_',' ').title()}\n",
        "                </h3>\n",
        "                <p style=\"margin:6px 0 0 0; font-weight:600;\">\n",
        "                    Confidence: {top['score']*100:.2f}%\n",
        "                </p>\n",
        "            </div>\n",
        "        \"\"\"))\n",
        "\n",
        "        # Save to Database\n",
        "        try:\n",
        "            db.collection(\"plant_images\").add({\n",
        "                \"plant\": a_name.value.strip(),\n",
        "                \"file\": fname,\n",
        "                \"prediction\": top[\"label\"],\n",
        "                \"score\": float(top[\"score\"]),\n",
        "                \"time\": datetime.now(timezone.utc),\n",
        "            })\n",
        "            print(\" Data saved .\")\n",
        "        except NameError:\n",
        "            print(\" Firestore client 'db' not defined ‚Äî skipping save.\")\n",
        "        except Exception as e:\n",
        "            print(\" Failed to save to Firestore:\", e)\n",
        "\n",
        "a_btn.on_click(run_plant_analysis)\n",
        "\n",
        "screenA = create_card(\n",
        "    \"Plant Diagnostic\",\n",
        "    \"Upload a plant leaf photo, detect possible disease, and store the result.\",\n",
        "    [\n",
        "        a_name,\n",
        "        widgets.HTML(\"<div style='height:8px;'></div>\"),\n",
        "        a_uploader,\n",
        "        a_hint,\n",
        "        widgets.HTML(\"<div style='height:12px;'></div>\"),\n",
        "        a_btn,\n",
        "        a_out,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# SCREEN B ‚Äî IOT DATA\n",
        "\n",
        "b_out = widgets.Output()\n",
        "# choose sensor\n",
        "\n",
        "b_feed = widgets.Dropdown(\n",
        "    options=[\"soil\", \"humidity\", \"temperature\"],\n",
        "    value=\"soil\",\n",
        "    description=\"Feed:\",\n",
        "    style={'description_width': '70px'},\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "# number of samples slider\n",
        "\n",
        "b_limit = widgets.IntSlider(\n",
        "    value=10, min=1, max=100, step=1,\n",
        "    description=\"Samples:\",\n",
        "    style={'description_width': '70px'},\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "# button to get data\n",
        "\n",
        "b_btn = widgets.Button(\n",
        "    description=\"Get Data\",\n",
        "    layout=widgets.Layout(width='50%', height='40px')\n",
        ")\n",
        "b_btn.add_class(\"btn-primary\")\n",
        "# fetch sensor history from your function\n",
        "\n",
        "def fetch_sensor_data(_):\n",
        "    with b_out:\n",
        "        clear_output()\n",
        "        try:\n",
        "            df = fetch_history(b_feed.value, b_limit.value)\n",
        "            print(f\"Rows returned: {len(df)}\")\n",
        "            display(df)\n",
        "            print(\"\\nLatest value:\", df[\"value\"].iloc[-1], \"| at:\", df[\"created_at\"].iloc[-1])\n",
        "        except NameError:\n",
        "            print(\"‚Ñπ 'fetch_history' is not defined.\")\n",
        "        except Exception as e:\n",
        "            print(\" Error fetching data:\", e)\n",
        "\n",
        "b_btn.on_click(fetch_sensor_data)\n",
        "\n",
        "screenB = create_card(\n",
        "    \"IoT Data\",\n",
        "    \"Fetch sensor history from the course server.\",\n",
        "    [\n",
        "        b_feed,\n",
        "        b_limit,\n",
        "        widgets.HTML(\"<div style='height:8px;'></div>\"),\n",
        "        b_btn,\n",
        "        b_out,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# SCREEN C ‚Äî DASHBOARD (Build + status + RAG)\n",
        "dash_out = widgets.Output()\n",
        "\n",
        "dash_feed = widgets.Dropdown(\n",
        "    options=[\"soil\", \"humidity\", \"temperature\"],\n",
        "    value=\"soil\",\n",
        "    description=\"Feed:\",\n",
        "    style={'description_width': '70px'},\n",
        "    layout=widgets.Layout(width='60%')\n",
        ")\n",
        "dash_limit = widgets.IntSlider(\n",
        "    value=30, min=10, max=200, step=10,\n",
        "    description=\"Samples:\",\n",
        "    style={'description_width': '70px'},\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "dash_btn = widgets.Button(\n",
        "    description=\"Build Dashboard\",\n",
        "    layout=widgets.Layout(width='60%', height='40px')\n",
        ")\n",
        "dash_btn.add_class(\"btn-warning\")\n",
        "# function to decide if condition is ok/warning/bad\n",
        "\n",
        "def dashboard_status(feed, latest):\n",
        "    if feed == \"soil\":\n",
        "        if latest < 30: return \"Critical\"\n",
        "        if latest < 45: return \"Warning\"\n",
        "        return \"Healthy\"\n",
        "    if feed == \"humidity\":\n",
        "        if latest < 30: return \"Warning\"\n",
        "        return \"OK\"\n",
        "    if feed == \"temperature\":\n",
        "        if latest < 10 or latest > 35: return \"Warning\"\n",
        "        return \"OK\"\n",
        "    return \"OK\"\n",
        "# build plot dashboard\n",
        "\n",
        "def build_dashboard(_):\n",
        "    with dash_out:\n",
        "        clear_output()\n",
        "        try:\n",
        "            df = fetch_history(dash_feed.value, dash_limit.value)\n",
        "        except NameError:\n",
        "            print(\" 'fetch_history' is not defined.\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            print(\" Error fetching data:\", e)\n",
        "            return\n",
        "\n",
        "        latest = df[\"value\"].iloc[-1]\n",
        "        status = dashboard_status(dash_feed.value, latest)\n",
        "        print(f\"Current Status: {status}\")\n",
        "\n",
        "        # advice feature\n",
        "        if status in [\"Critical\", \"Warning\"]:\n",
        "            print(\"\\n Generating research-based insight...\")\n",
        "            if dash_feed.value == \"soil\":\n",
        "                query = \"impact of low soil moisture and water stress on plant disease\"\n",
        "            elif dash_feed.value == \"humidity\":\n",
        "                query = \"how low humidity affects plant health and fungal growth\"\n",
        "            else:\n",
        "                query = f\"effects of extreme {dash_feed.value} on plant pathology\"\n",
        "\n",
        "            try:\n",
        "                insight = rag_answer_without_llm(query, top_k=1)\n",
        "                print(\"=\" * 60)\n",
        "                print(\" SMART INSIGHT:\")\n",
        "                print(insight)\n",
        "                print(\"=\" * 60)\n",
        "            except NameError:\n",
        "                print(\" 'rag_answer_without_llm' not defined, skipping insight.\")\n",
        "            except Exception as e:\n",
        "                print(\" Error calling RAG function:\", e)\n",
        "\n",
        "       # plot values\n",
        "        plt.figure(figsize=(9, 3))\n",
        "        plt.plot(df[\"created_at\"], df[\"value\"], marker=\"o\")\n",
        "        plt.title(f\"{dash_feed.value.title()} Monitoring ‚Äì Status: {status}\")\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Value\")\n",
        "        plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "dash_btn.on_click(build_dashboard)\n",
        "\n",
        "screenC = create_card(\n",
        "    \"Dashboard\",\n",
        "    \"Build a visual dashboard with status and smart research insight.\",\n",
        "    [\n",
        "        dash_feed,\n",
        "        dash_limit,\n",
        "        widgets.HTML(\"<div style='height:8px;'></div>\"),\n",
        "        dash_btn,\n",
        "        dash_out,\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# SCREEN D ‚Äî SEARCH (FIRESTORE INDEX)\n",
        "\n",
        "c_out = widgets.Output()\n",
        "\n",
        "index_box = widgets.Text(\n",
        "    value=\"inverted_index\",\n",
        "    description=\"Index:\",\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width=\"70%\")\n",
        ")\n",
        "query_box = widgets.Text(\n",
        "    value=\"about\",\n",
        "    description=\"Search:\",\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width=\"70%\")\n",
        ")\n",
        "\n",
        "search_btn = widgets.Button(\n",
        "    description=\"Search database\",\n",
        "    layout=widgets.Layout(width='50%', height='40px', margin='8px 0 0 80px')\n",
        ")\n",
        "search_btn.add_class(\"btn-primary\")\n",
        "# search data if exists\n",
        "def search_inverted_index(index_name: str, term: str):\n",
        "    index_name = index_name.strip()\n",
        "    term = term.strip().lower()\n",
        "    if not index_name or not term:\n",
        "        return None, \"Enter both Index and Search.\"\n",
        "\n",
        "    doc = db.collection(index_name).document(term).get()\n",
        "    if doc.exists:\n",
        "        data = doc.to_dict() or {}\n",
        "        return {\n",
        "            \"term\": term,\n",
        "            \"df\": data.get(\"df\"),\n",
        "            \"doc_ids\": data.get(\"doc_ids\", [])\n",
        "        }, None\n",
        "\n",
        "    qs = list(db.collection(index_name)\n",
        "              .where(\"term\", \"==\", term)\n",
        "              .limit(1)\n",
        "              .stream())\n",
        "    if qs:\n",
        "        data = qs[0].to_dict() or {}\n",
        "        return {\n",
        "            \"term\": term,\n",
        "            \"df\": data.get(\"df\"),\n",
        "            \"doc_ids\": data.get(\"doc_ids\", [])\n",
        "        }, None\n",
        "\n",
        "    return None, f\"No results for '{term}' in '{index_name}'.\"\n",
        "\n",
        "def on_search_click(_):\n",
        "    with c_out:\n",
        "        clear_output()\n",
        "        try:\n",
        "            result, err = search_inverted_index(index_box.value, query_box.value)\n",
        "        except NameError:\n",
        "            print(\"‚ÑπÔ∏è Firestore client 'db' is not defined.\")\n",
        "            return\n",
        "\n",
        "        if err:\n",
        "            print(err)\n",
        "            return\n",
        "\n",
        "        html = \"<ul style='padding-left:18px;'>\"\n",
        "        for did in result.get(\"doc_ids\", []):\n",
        "            html += f\"<li>{did}</li>\"\n",
        "        html += \"</ul>\"\n",
        "\n",
        "        display(widgets.HTML(f\"\"\"\n",
        "            <p><b>term:</b> {result['term']}</p>\n",
        "            <p><b>df:</b> {result.get('df','')}</p>\n",
        "            <p><b>doc_ids:</b></p>\n",
        "            {html}\n",
        "        \"\"\"))\n",
        "\n",
        "search_btn.on_click(on_search_click)\n",
        "\n",
        "screenD = create_card(\n",
        "    \"Knowledge Base Search\",\n",
        "    \"Search terms in the Firestore inverted index.\",\n",
        "    [\n",
        "        index_box,\n",
        "        query_box,\n",
        "        search_btn,\n",
        "        c_out,\n",
        "    ]\n",
        ")\n",
        "\n",
        "#\n",
        "# APP SHELL + TABS\n",
        "tabs = widgets.Tab(children=[screenA, screenB, screenC, screenD])\n",
        "tabs.set_title(0, \"Diagnosis\")\n",
        "tabs.set_title(1, \"IoT Data\")\n",
        "tabs.set_title(2, \"Dashboard\")\n",
        "tabs.set_title(3, \"Search\")\n",
        "\n",
        "header = widgets.HTML(\n",
        "    \"<div style='background:#1e40af; color:white; padding:18px 20px;\"\n",
        "    \"border-radius:12px 12px 0 0; font-size:22px; font-weight:700;'>\"\n",
        "    \"üå± PlantCare  Pro</div>\"\n",
        ")\n",
        "\n",
        "app = widgets.VBox([tabs])\n",
        "app.add_class(\"app-shell\")\n",
        "\n",
        "display(header)\n",
        "display(app)\n"
      ],
      "metadata": {
        "id": "gBdLeLCp545B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}