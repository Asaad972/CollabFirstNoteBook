{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6Iby1aMyzx5HKDJ8OlawV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asaad972/CollabFirstNoteBook/blob/main/HW02_Cloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZcwcS_-VsXcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce56213-51aa-4c6c-97f5-4fd625406d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dependencies ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: Minimal package installation (only if missing)\n",
        "import importlib.util, sys, subprocess\n",
        "\n",
        "def ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "# Usually already installed in Colab, but keep safe:\n",
        "ensure(\"pandas\", \"pandas\")\n",
        "\n",
        "# Required for your homework plan:\n",
        "ensure(\"nltk\", \"nltk\")\n",
        "ensure(\"sentence-transformers\", \"sentence_transformers\")\n",
        "ensure(\"faiss-cpu\", \"faiss\")\n",
        "ensure(\"pymupdf\", \"fitz\")\n",
        "\n",
        "\n",
        "print(\" Dependencies ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Imports + NLTK resources (run once per runtime)\n",
        "\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# NLTK downloads (required for stopwords/tokenizer/lemmatizer)\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "print(\" Imports ready + NLTK resources downloaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wDldDHHb8tY",
        "outputId": "8eb493cf-b71b-4b7d-8d09-2c1304f0df1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Imports ready + NLTK resources downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Store Classes (Vector Store + Inverted Index)\n",
        "# =====================================================\n",
        "\"\"\"\n",
        " CELL 3: STORE CLASSES\n",
        "- SimpleVectorStore: stores embeddings + documents + metadatas + ids (like Tirgul 7)\n",
        "- InvertedIndexStore: stores required index schema term -> DocIDs (homework requirement)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- Vector Store (similar to Tirgul 7) ----------\n",
        "class SimpleVectorStore:\n",
        "    \"\"\"Simple in-memory vector store (fallback)\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.documents = []\n",
        "        self.embeddings = []   # list of numpy arrays\n",
        "        self.metadatas = []\n",
        "        self.ids = []\n",
        "        print(\" SimpleVectorStore initialized\")\n",
        "\n",
        "    def add(self, embeddings, documents, metadatas, ids):\n",
        "        # Ensure numpy arrays\n",
        "        embeddings = [np.asarray(e, dtype=np.float32) for e in embeddings]\n",
        "        self.embeddings.extend(embeddings)\n",
        "        self.documents.extend(documents)\n",
        "        self.metadatas.extend(metadatas)\n",
        "        self.ids.extend(ids)\n",
        "        print(f\" Added {len(documents)} documents to simple vector store\")\n",
        "\n",
        "    def query(self, query_embeddings, n_results=5):\n",
        "        if not self.embeddings:\n",
        "            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}\n",
        "\n",
        "        q = np.asarray(query_embeddings[0], dtype=np.float32)\n",
        "\n",
        "        E = np.vstack(self.embeddings)  # shape: (N, d)\n",
        "\n",
        "        # cosine similarity without sklearn\n",
        "        q_norm = np.linalg.norm(q) + 1e-12\n",
        "        E_norm = np.linalg.norm(E, axis=1) + 1e-12\n",
        "        sims = (E @ q) / (E_norm * q_norm)\n",
        "\n",
        "        top_idx = np.argsort(sims)[::-1][:n_results]\n",
        "\n",
        "        return {\n",
        "            'ids': [[self.ids[i] for i in top_idx]],\n",
        "            'documents': [[self.documents[i] for i in top_idx]],\n",
        "            'metadatas': [[self.metadatas[i] for i in top_idx]],\n",
        "            'distances': [[float(1 - sims[i]) for i in top_idx]]  # distance-like\n",
        "        }\n",
        "\n",
        "    def count(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "\n",
        "# ---------- Inverted Index (required by homework) ----------\n",
        "class InvertedIndexStore:\n",
        "    \"\"\"Required structure: term -> DocIDs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.term_to_docids = defaultdict(set)\n",
        "        print(\" InvertedIndexStore initialized\")\n",
        "\n",
        "    def add_occurrence(self, term: str, doc_id: str):\n",
        "        self.term_to_docids[term].add(doc_id)\n",
        "\n",
        "    def get_docids(self, term: str):\n",
        "        return sorted(self.term_to_docids.get(term, set()))\n",
        "\n",
        "    def count_terms(self) -> int:\n",
        "        return len(self.term_to_docids)\n",
        "\n",
        "    def to_required_format(self):\n",
        "        # [{\"term\": ..., \"DocIDs\": [...]}, ...]\n",
        "        return [{\"term\": t, \"DocIDs\": sorted(list(docids))}\n",
        "                for t, docids in sorted(self.term_to_docids.items())]\n",
        "\n",
        "\n",
        "print(\" Store classes defined!\")\n",
        "print(\" Next: Cell 4 (core logic: preprocess + build index + embeddings)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6h6ichmcgHy",
        "outputId": "3cd93d17-e97e-47a5-b45a-034bbd66ccca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Store classes defined!\n",
            " Next: Cell 4 (core logic: preprocess + build index + embeddings)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Core setup (custom stopwords + stemming + embedding model + FAISS)\n",
        "\n",
        "# --- Custom stopwords (you define them) ---\n",
        "# We remove these words because they are very frequent function words (articles, prepositions, pronouns).\n",
        "# They usually do not add topic meaning, but they increase index size and add noise to retrieval.\n",
        "CUSTOM_STOPWORDS = {\n",
        "    \"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\n",
        "    \"to\",\"of\",\"in\",\"on\",\"at\",\"for\",\"from\",\"by\",\"with\",\"as\",\n",
        "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"this\",\"that\",\"these\",\"those\",\n",
        "    \"it\",\"its\",\"they\",\"them\",\"their\",\"we\",\"our\",\"you\",\"your\",\n",
        "    \"i\",\"me\",\"my\",\"he\",\"him\",\"his\",\"she\",\"her\",\n",
        "    \"not\",\"no\",\"do\",\"does\",\"did\",\"doing\"\n",
        "}\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    \"\"\"\n",
        "    Returns list of terms for indexing:\n",
        "    - lowercase\n",
        "    - tokenize\n",
        "    - keep alphabetic tokens only\n",
        "    - remove custom stopwords\n",
        "    - apply stemming\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    terms = []\n",
        "    for tok in tokens:\n",
        "        if tok.isalpha() and tok not in CUSTOM_STOPWORDS:\n",
        "            terms.append(stemmer.stem(tok))\n",
        "    return terms\n",
        "\n",
        "# --- Embedding model (for semantic retrieval) ---\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# --- FAISS index (stores embeddings for doc-level retrieval) ---\n",
        "faiss_index = None\n",
        "vector_dim = None\n",
        "\n",
        "# Parallel stores (FAISS row -> doc data)\n",
        "vector_doc_ids = []   # doc_id\n",
        "vector_texts = []     # full doc text\n",
        "\n",
        "print(\" Core setup ready (custom stopwords + stemming + embeddings + FAISS)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ly5DF07crLF",
        "outputId": "94e660b1-0dbc-4619-86fd-2c8fe9de359b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Core setup ready (custom stopwords + stemming + embeddings + FAISS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Wikipedia source links (seed documents for the corpus)\n",
        "\n",
        "wiki_links = [\n",
        "    \"https://en.wikipedia.org/wiki/Plant_disease\",\n",
        "    \"https://en.wikipedia.org/wiki/Plant_pathology\",\n",
        "    \"https://en.wikipedia.org/wiki/Fungus\",\n",
        "    \"https://en.wikipedia.org/wiki/Bacterial_wilt\",\n",
        "    \"https://en.wikipedia.org/wiki/Powdery_mildew\"\n",
        "]\n",
        "\n",
        "print(\"Wikipedia links used:\")\n",
        "for i, link in enumerate(wiki_links, 1):\n",
        "    print(f\"{i}. {link}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5Swqy3GdK37",
        "outputId": "abd48a9f-7a40-4e7d-e0f1-c2f9e5a7174b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wikipedia links used:\n",
            "1. https://en.wikipedia.org/wiki/Plant_disease\n",
            "2. https://en.wikipedia.org/wiki/Plant_pathology\n",
            "3. https://en.wikipedia.org/wiki/Fungus\n",
            "4. https://en.wikipedia.org/wiki/Bacterial_wilt\n",
            "5. https://en.wikipedia.org/wiki/Powdery_mildew\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Load documents from Wikipedia (API fetch + normalization + metadata)\n",
        "\n",
        "import requests\n",
        "import re\n",
        "\n",
        "WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "# Wikipedia blocks requests without a proper User-Agent sometimes\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"HW02-Cloud-RAG/1.0 (student project; contact: student@example.com)\"\n",
        "}\n",
        "\n",
        "def title_from_wiki_url(url: str) -> str:\n",
        "    if \"/wiki/\" not in url:\n",
        "        raise ValueError(f\"Unsupported Wikipedia URL: {url}\")\n",
        "    title = url.split(\"/wiki/\", 1)[1]\n",
        "    title = title.split(\"#\", 1)[0]      # remove anchors\n",
        "    title = title.replace(\"_\", \" \")\n",
        "    return title\n",
        "\n",
        "def fetch_page_extract_by_title(title: str):\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"prop\": \"extracts|info\",\n",
        "        \"titles\": title,\n",
        "        \"inprop\": \"url\",\n",
        "        \"explaintext\": True,\n",
        "        \"redirects\": 1,   # follow redirects\n",
        "        \"origin\": \"*\"     # helps in some environments\n",
        "    }\n",
        "    r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=30)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    pages = r.json()[\"query\"][\"pages\"]\n",
        "    page = next(iter(pages.values()))\n",
        "\n",
        "    # Handle missing page\n",
        "    if \"missing\" in page:\n",
        "        return {\"pageid\": None, \"title\": title, \"url\": \"\", \"text\": \"\"}\n",
        "\n",
        "    return {\n",
        "        \"pageid\": page.get(\"pageid\"),\n",
        "        \"title\": page.get(\"title\", title),\n",
        "        \"url\": page.get(\"fullurl\", \"\"),\n",
        "        \"text\": page.get(\"extract\", \"\")\n",
        "    }\n",
        "\n",
        "def slugify(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s)\n",
        "    return s.strip(\"-\")\n",
        "\n",
        "def load_docs_from_wiki_links(wiki_links):\n",
        "    docs = {}\n",
        "    docs_meta = {}\n",
        "\n",
        "    for url in wiki_links:\n",
        "        title = title_from_wiki_url(url)\n",
        "        data = fetch_page_extract_by_title(title)\n",
        "\n",
        "        text = (data.get(\"text\") or \"\").strip()\n",
        "        if not text:\n",
        "            print(f\"Empty/blocked page: {title} | {url}\")\n",
        "            continue\n",
        "\n",
        "        doc_id = f\"wiki_{slugify(data['title'])}\"\n",
        "        docs[doc_id] = text\n",
        "        docs_meta[doc_id] = {\n",
        "            \"title\": data[\"title\"],\n",
        "            \"url\": data.get(\"url\") or url,\n",
        "            \"source\": \"wikipedia\",\n",
        "            \"pageid\": data.get(\"pageid\"),\n",
        "        }\n",
        "\n",
        "        print(f\"Loaded: {data['title']} -> {doc_id} | chars={len(text)}\")\n",
        "\n",
        "    return docs, docs_meta\n",
        "\n",
        "docs, docs_meta = load_docs_from_wiki_links(wiki_links)\n",
        "print(\"Docs loaded:\", len(docs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka2aEOAOgS2_",
        "outputId": "6c22ccef-61e4-4a6c-cabf-c4b80ba9fc26"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: Plant disease -> wiki_plant-disease | chars=9654\n",
            "Loaded: Plant pathology -> wiki_plant-pathology | chars=5228\n",
            "Loaded: Fungus -> wiki_fungus | chars=65562\n",
            "Loaded: Bacterial wilt -> wiki_bacterial-wilt | chars=3688\n",
            "Loaded: Erysiphaceae -> wiki_erysiphaceae | chars=14230\n",
            "Docs loaded: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Build the required index (term -> DocIDs) + build FAISS embeddings store (doc-level)\n",
        "\n",
        "# 1) Build inverted index (term -> DocIDs)\n",
        "inv_index = InvertedIndexStore()\n",
        "\n",
        "for doc_id, text in docs.items():\n",
        "    terms = preprocess_text(text)   # uses custom stopwords + stemming\n",
        "    for t in set(terms):            # presence only (not frequency)\n",
        "        inv_index.add_occurrence(t, doc_id)\n",
        "\n",
        "print(f\" Inverted index built. Unique terms: {inv_index.count_terms()}\")\n",
        "\n",
        "# 2) Build embeddings + FAISS (one vector per doc)\n",
        "doc_ids = list(docs.keys())\n",
        "texts = [docs[d] for d in doc_ids]\n",
        "\n",
        "emb = embed_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "\n",
        "vector_dim = emb.shape[1]\n",
        "faiss_index = faiss.IndexFlatIP(vector_dim)  # cosine similarity via normalized embeddings\n",
        "faiss_index.add(emb)\n",
        "\n",
        "# parallel arrays for retrieval results\n",
        "vector_doc_ids = doc_ids\n",
        "vector_texts = texts\n",
        "\n",
        "print(f\" FAISS built. Vectors: {faiss_index.ntotal} | dim={vector_dim}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyo0irOJLhRE",
        "outputId": "f202bc9a-eefc-42fb-9e4d-d9f717ef2961"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " InvertedIndexStore initialized\n",
            " Inverted index built. Unique terms: 2580\n",
            " FAISS built. Vectors: 5 | dim=384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Firebase / Firestore initialization (cloud persistence setup)\n",
        "\n",
        "!pip -q install firebase-admin\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "\n",
        "# Initialize Firebase Admin SDK using a service account key\n",
        "cred = credentials.Certificate(\n",
        "    \"hw02-cloud-inverted-index-firebase-adminsdk-fbsvc-437db7abaa.json\"\n",
        ")\n",
        "\n",
        "# Prevent re-initialization errors in notebook environments\n",
        "if not firebase_admin._apps:\n",
        "    firebase_admin.initialize_app(cred)\n",
        "\n",
        "# Create Firestore client\n",
        "db = firestore.client()\n",
        "print(\"Firestore connected:\", db.project)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3PfU1boz_l-",
        "outputId": "9ac405a9-86dc-45bb-c7f8-49414a349567"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Firestore connected: hw02-cloud-inverted-index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Upload inverted index to Firestore (cloud storage of term → DocIDs)\n",
        "\n",
        "from google.cloud.firestore_v1 import ArrayUnion\n",
        "\n",
        "def upload_inverted_index(inv_index, collection_name=\"inverted_index\", batch_size=400):\n",
        "    \"\"\"\n",
        "    Uploads the inverted index to Firestore.\n",
        "\n",
        "    Each term is stored as a document in the collection:\n",
        "      inverted_index/{term}\n",
        "\n",
        "    Stored fields:\n",
        "      - term     : the stemmed term\n",
        "      - doc_ids  : list of document IDs containing the term\n",
        "      - df       : document frequency (number of documents containing the term)\n",
        "\n",
        "    Batch writes are used to stay within Firestore limits and improve performance.\n",
        "    \"\"\"\n",
        "    col = db.collection(collection_name)\n",
        "    records = inv_index.to_required_format()  # [{\"term\": ..., \"DocIDs\": [...]}, ...]\n",
        "\n",
        "    batch = db.batch()\n",
        "    ops = 0\n",
        "\n",
        "    for r in records:\n",
        "        term = r[\"term\"]\n",
        "        doc_ids = r[\"DocIDs\"]\n",
        "\n",
        "        # Use the term itself as the Firestore document ID (trimmed for safety)\n",
        "        doc_id = term[:1500]\n",
        "\n",
        "        ref = col.document(doc_id)\n",
        "        batch.set(ref, {\n",
        "            \"term\": term,\n",
        "            \"doc_ids\": doc_ids,\n",
        "            \"df\": len(doc_ids),\n",
        "        })\n",
        "\n",
        "        ops += 1\n",
        "        if ops >= batch_size:\n",
        "            batch.commit()\n",
        "            batch = db.batch()\n",
        "            ops = 0\n",
        "\n",
        "    # Commit any remaining operations\n",
        "    if ops > 0:\n",
        "        batch.commit()\n",
        "\n",
        "    print(f\"Uploaded {len(records)} terms to Firestore collection '{collection_name}'\")\n",
        "\n",
        "# Upload the built inverted index\n",
        "upload_inverted_index(inv_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GMKBFBw1sdE",
        "outputId": "a04ba6a1-b528-4949-89dc-64d4760f73f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded 2580 terms to Firestore collection 'inverted_index'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Upload Wikipedia document metadata to Firestore (documents collection)\n",
        "\n",
        "def upload_wiki_meta(docs_meta, collection_name=\"documents\", batch_size=400):\n",
        "    \"\"\"\n",
        "    Uploads Wikipedia document metadata to Firestore.\n",
        "\n",
        "    Each document is stored as:\n",
        "      documents/{doc_id}\n",
        "\n",
        "    Stored fields:\n",
        "      - doc_id  : your internal document ID (e.g., wiki_plant-disease)\n",
        "      - title   : Wikipedia page title\n",
        "      - url     : Wikipedia page URL\n",
        "      - source  : \"wikipedia\"\n",
        "      - pageid  : Wikipedia page id (if available)\n",
        "\n",
        "    This does NOT upload the full article text; it only uploads metadata.\n",
        "    \"\"\"\n",
        "    col = db.collection(collection_name)\n",
        "\n",
        "    batch = db.batch()\n",
        "    ops = 0\n",
        "\n",
        "    for doc_id, meta in docs_meta.items():\n",
        "        ref = col.document(doc_id)\n",
        "        batch.set(ref, {\n",
        "            \"doc_id\": doc_id,\n",
        "            \"title\": meta.get(\"title\", \"\"),\n",
        "            \"url\": meta.get(\"url\", \"\"),\n",
        "            \"source\": meta.get(\"source\", \"wikipedia\"),\n",
        "            \"pageid\": meta.get(\"pageid\", None),\n",
        "        }, merge=True)\n",
        "\n",
        "        ops += 1\n",
        "        if ops >= batch_size:\n",
        "            batch.commit()\n",
        "            batch = db.batch()\n",
        "            ops = 0\n",
        "\n",
        "    if ops > 0:\n",
        "        batch.commit()\n",
        "\n",
        "    print(f\"Uploaded {len(docs_meta)} wiki docs to '{collection_name}'\")\n",
        "\n",
        "# Upload metadata for the loaded Wikipedia docs\n",
        "upload_wiki_meta(docs_meta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al003f-n5Ubq",
        "outputId": "d71227b3-021d-4c54-8b82-98e222078100"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded 5 wiki docs to 'documents'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Embedding-based document retrieval using FAISS (semantic search)\n",
        "\n",
        "def retrieve_top_docs(query: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Retrieves the top-K most relevant documents for a user query using\n",
        "    vector embeddings and FAISS similarity search.\n",
        "\n",
        "    This function:\n",
        "      1) Embeds the query using the same embedding model as the documents\n",
        "      2) Searches the FAISS index using cosine similarity\n",
        "      3) Returns ranked documents with titles, similarity scores, and text snippets\n",
        "\n",
        "    Note: This is retrieval only (no generation / no LLM).\n",
        "    \"\"\"\n",
        "    if faiss_index is None or faiss_index.ntotal == 0:\n",
        "        return \"FAISS index is empty. Build vectors first.\"\n",
        "\n",
        "    # Embed and normalize the query\n",
        "    q_emb = embed_model.encode(\n",
        "        [query],\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    # Search FAISS index\n",
        "    distances, indices = faiss_index.search(q_emb, top_k)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"Query: {query}\")\n",
        "    lines.append(\"=\" * 60)\n",
        "\n",
        "    # Format ranked results\n",
        "    for rank, idx in enumerate(indices[0], start=1):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "\n",
        "        doc_id = vector_doc_ids[idx]\n",
        "        title = docs_meta.get(doc_id, {}).get(\"title\", \"\")\n",
        "        text = vector_texts[idx]\n",
        "        snippet = re.sub(r\"\\s+\", \" \", text)[:350]\n",
        "        score = float(distances[0][rank - 1])\n",
        "\n",
        "        lines.append(f\"{rank}) {doc_id} | {title} | similarity: {score:.4f}\")\n",
        "        lines.append(f\"Snippet: {snippet}...\")\n",
        "        lines.append(\"-\" * 60)\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "print(\"Retrieval function ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDJK3udPfGLt",
        "outputId": "a0e98828-d154-4a48-dc75-92353051a1cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: RAG-style output (retrieval + \"enriched\" answer without OpenAI)\n",
        "# We will: retrieve top docs, then produce a simple enriched response by extracting key sentences.\n",
        "\n",
        "def split_sentences(text: str):\n",
        "    # simple sentence split (good enough for baseline)\n",
        "    parts = re.split(r'(?<=[.!?])\\s+', re.sub(r\"\\s+\", \" \", text).strip())\n",
        "    return [s for s in parts if len(s) > 30]\n",
        "\n",
        "def rag_answer_without_llm(query: str, top_k: int = 3, max_sentences_per_doc: int = 2):\n",
        "    if faiss_index is None or faiss_index.ntotal == 0:\n",
        "        return \"FAISS index is empty. Build vectors first.\"\n",
        "\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "    distances, indices = faiss_index.search(q_emb, top_k)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"Query: {query}\")\n",
        "    lines.append(\"=\" * 60)\n",
        "\n",
        "    # Retrieval section\n",
        "    lines.append(\"Top retrieved documents:\")\n",
        "    retrieved = []\n",
        "    for rank, idx in enumerate(indices[0], start=1):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        doc_id = vector_doc_ids[idx]\n",
        "        title = docs_meta.get(doc_id, {}).get(\"title\", \"\")\n",
        "        score = float(distances[0][rank - 1])\n",
        "        retrieved.append((doc_id, title, score))\n",
        "        lines.append(f\"{rank}) {doc_id} | {title} | similarity: {score:.4f}\")\n",
        "    lines.append(\"=\" * 60)\n",
        "\n",
        "    # Enriched response (extractive, no LLM)\n",
        "    lines.append(\"Enriched response (extractive, no LLM):\")\n",
        "    q_terms = set(preprocess_text(query))\n",
        "\n",
        "    for doc_id, title, score in retrieved:\n",
        "        text = docs[doc_id]\n",
        "        sents = split_sentences(text)\n",
        "\n",
        "        # score sentences by overlap with query terms (stems)\n",
        "        scored = []\n",
        "        for s in sents:\n",
        "            s_terms = set(preprocess_text(s))\n",
        "            overlap = len(q_terms & s_terms)\n",
        "            if overlap > 0:\n",
        "                scored.append((overlap, s))\n",
        "\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "        best = [s for _, s in scored[:max_sentences_per_doc]]\n",
        "\n",
        "        lines.append(f\"- Source: {doc_id} | {title}\")\n",
        "        if best:\n",
        "            for b in best:\n",
        "                lines.append(f\"  • {b}\")\n",
        "        else:\n",
        "            lines.append(\"  • (No strong matching sentences found)\")\n",
        "        lines.append(\"-\" * 60)\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "print(\" RAG-style (no OpenAI) function ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIPiXHAolYpt",
        "outputId": "ba75851a-cb12-4e0c-da5f-e72334461b0a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " RAG-style (no OpenAI) function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Quick demo (edit the query text)\n",
        "\n",
        "print(retrieve_top_docs(\"how to detect plant diseases using sensors and ai\", top_k=3))\n",
        "print()\n",
        "print(rag_answer_without_llm(\"how to detect plant diseases using sensors and ai\", top_k=3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9-53EC4lxNh",
        "outputId": "2372081c-8241-47fe-ae0e-2b5fed5a2979"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: how to detect plant diseases using sensors and ai\n",
            "============================================================\n",
            "1) wiki_plant-disease | Plant disease | similarity: 0.4522\n",
            "Snippet: Plant diseases are diseases in plants caused by pathogens (infectious organisms) and environmental conditions (physiological factors). Organisms that cause infectious disease include fungi, oomycetes, bacteria, viruses, viroids, virus-like organisms, phytoplasmas, protozoa, nematodes and parasitic plants. Not included are ectoparasites like insects...\n",
            "------------------------------------------------------------\n",
            "2) wiki_plant-pathology | Plant pathology | similarity: 0.4479\n",
            "Snippet: Plant pathology or phytopathology is the scientific study of plant diseases caused by pathogens (infectious organisms) and environmental conditions (physiological factors). Plant pathology involves the study of pathogen identification, disease etiology, disease cycles, economic impact, plant disease epidemiology, plant disease resistance, how plant...\n",
            "------------------------------------------------------------\n",
            "3) wiki_bacterial-wilt | Bacterial wilt | similarity: 0.3261\n",
            "Snippet: Bacterial wilt is a complex of diseases that occur in plants such as Cucurbitaceae and Solanaceae (tomato etc.) and are caused by the pathogens Erwinia tracheiphila, a gram-negative bacterium, or Curtobacterium flaccumfaciens pv. flaccumfaciens, a gram-positive bacterium. Cucumber and melon plants are most susceptible, but squash, pumpkins, and gou...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Query: how to detect plant diseases using sensors and ai\n",
            "============================================================\n",
            "Top retrieved documents:\n",
            "1) wiki_plant-disease | Plant disease | similarity: 0.4522\n",
            "2) wiki_plant-pathology | Plant pathology | similarity: 0.4479\n",
            "3) wiki_bacterial-wilt | Bacterial wilt | similarity: 0.3261\n",
            "============================================================\n",
            "Enriched response (extractive, no LLM):\n",
            "- Source: wiki_plant-disease | Plant disease\n",
            "  • To solve this, new methods are needed to detect diseases and pests early, such as novel sensors that detect plant odours and spectroscopy and biophotonics that are able to diagnose plant health and metabolism.\n",
            "  • Plant diseases are diseases in plants caused by pathogens (infectious organisms) and environmental conditions (physiological factors).\n",
            "------------------------------------------------------------\n",
            "- Source: wiki_plant-pathology | Plant pathology\n",
            "  • Plant pathology involves the study of pathogen identification, disease etiology, disease cycles, economic impact, plant disease epidemiology, plant disease resistance, how plant diseases affect humans and animals, pathosystem genetics, and management of plant diseases.\n",
            "  • Plant pathology or phytopathology is the scientific study of plant diseases caused by pathogens (infectious organisms) and environmental conditions (physiological factors).\n",
            "------------------------------------------------------------\n",
            "- Source: wiki_bacterial-wilt | Bacterial wilt\n",
            "  • Bacterial wilt is a complex of diseases that occur in plants such as Cucurbitaceae and Solanaceae (tomato etc.) and are caused by the pathogens Erwinia tracheiphila, a gram-negative bacterium, or Curtobacterium flaccumfaciens pv.\n",
            "  • == Disease transmission == Erwinia tracheiphila is spread between plants by two species of insect vectors, striped cucumber beetles (Acalymma vittatum) and spotted cucumber beetles (Diabrotica undecimpunctata).\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}