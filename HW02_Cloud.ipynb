{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asaad972/CollabFirstNoteBook/blob/main/HW02_Cloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcwcS_-VsXcf"
      },
      "outputs": [],
      "source": [
        "# CELL 1: Minimal package installation (only if missing)\n",
        "import importlib.util, sys, subprocess\n",
        "\n",
        "def ensure(pkg, import_name=None):\n",
        "    name = import_name or pkg\n",
        "    if importlib.util.find_spec(name) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "# Usually already installed in Colab, but keep safe:\n",
        "ensure(\"pandas\", \"pandas\")\n",
        "\n",
        "# Required for your homework plan:\n",
        "ensure(\"nltk\", \"nltk\")\n",
        "ensure(\"sentence-transformers\", \"sentence_transformers\")\n",
        "ensure(\"faiss-cpu\", \"faiss\")\n",
        "ensure(\"pymupdf\", \"fitz\")\n",
        "\n",
        "\n",
        "print(\" Dependencies ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Imports + NLTK resources (run once per runtime)\n",
        "\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# NLTK downloads (required for stopwords/tokenizer/lemmatizer)\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "print(\" Imports ready + NLTK resources downloaded\")\n"
      ],
      "metadata": {
        "id": "2wDldDHHb8tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Store Classes (Vector Store + Inverted Index)\n",
        "# =====================================================\n",
        "\"\"\"\n",
        " CELL 3: STORE CLASSES\n",
        "- SimpleVectorStore: stores embeddings + documents + metadatas + ids (like Tirgul 7)\n",
        "- InvertedIndexStore: stores required index schema term -> DocIDs (homework requirement)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------- Vector Store (similar to Tirgul 7) ----------\n",
        "class SimpleVectorStore:\n",
        "    \"\"\"Simple in-memory vector store (fallback)\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.documents = []\n",
        "        self.embeddings = []   # list of numpy arrays\n",
        "        self.metadatas = []\n",
        "        self.ids = []\n",
        "        print(\" SimpleVectorStore initialized\")\n",
        "\n",
        "    def add(self, embeddings, documents, metadatas, ids):\n",
        "        # Ensure numpy arrays\n",
        "        embeddings = [np.asarray(e, dtype=np.float32) for e in embeddings]\n",
        "        self.embeddings.extend(embeddings)\n",
        "        self.documents.extend(documents)\n",
        "        self.metadatas.extend(metadatas)\n",
        "        self.ids.extend(ids)\n",
        "        print(f\" Added {len(documents)} documents to simple vector store\")\n",
        "\n",
        "    def query(self, query_embeddings, n_results=5):\n",
        "        if not self.embeddings:\n",
        "            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}\n",
        "\n",
        "        q = np.asarray(query_embeddings[0], dtype=np.float32)\n",
        "\n",
        "        E = np.vstack(self.embeddings)  # shape: (N, d)\n",
        "\n",
        "        # cosine similarity without sklearn\n",
        "        q_norm = np.linalg.norm(q) + 1e-12\n",
        "        E_norm = np.linalg.norm(E, axis=1) + 1e-12\n",
        "        sims = (E @ q) / (E_norm * q_norm)\n",
        "\n",
        "        top_idx = np.argsort(sims)[::-1][:n_results]\n",
        "\n",
        "        return {\n",
        "            'ids': [[self.ids[i] for i in top_idx]],\n",
        "            'documents': [[self.documents[i] for i in top_idx]],\n",
        "            'metadatas': [[self.metadatas[i] for i in top_idx]],\n",
        "            'distances': [[float(1 - sims[i]) for i in top_idx]]  # distance-like\n",
        "        }\n",
        "\n",
        "    def count(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "\n",
        "# ---------- Inverted Index (required by homework) ----------\n",
        "class InvertedIndexStore:\n",
        "    \"\"\"Required structure: term -> DocIDs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.term_to_docids = defaultdict(set)\n",
        "        print(\" InvertedIndexStore initialized\")\n",
        "\n",
        "    def add_occurrence(self, term: str, doc_id: str):\n",
        "        self.term_to_docids[term].add(doc_id)\n",
        "\n",
        "    def get_docids(self, term: str):\n",
        "        return sorted(self.term_to_docids.get(term, set()))\n",
        "\n",
        "    def count_terms(self) -> int:\n",
        "        return len(self.term_to_docids)\n",
        "\n",
        "    def to_required_format(self):\n",
        "        # [{\"term\": ..., \"DocIDs\": [...]}, ...]\n",
        "        return [{\"term\": t, \"DocIDs\": sorted(list(docids))}\n",
        "                for t, docids in sorted(self.term_to_docids.items())]\n",
        "\n",
        "\n",
        "print(\" Store classes defined!\")\n",
        "print(\" Next: Cell 4 (core logic: preprocess + build index + embeddings)\")\n"
      ],
      "metadata": {
        "id": "H6h6ichmcgHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Core setup (custom stopwords + stemming + embedding model + FAISS)\n",
        "\n",
        "# --- Custom stopwords (you define them) ---\n",
        "# We remove these words because they are very frequent function words (articles, prepositions, pronouns).\n",
        "# They usually do not add topic meaning, but they increase index size and add noise to retrieval.\n",
        "CUSTOM_STOPWORDS = {\n",
        "    \"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\n",
        "    \"to\",\"of\",\"in\",\"on\",\"at\",\"for\",\"from\",\"by\",\"with\",\"as\",\n",
        "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
        "    \"this\",\"that\",\"these\",\"those\",\n",
        "    \"it\",\"its\",\"they\",\"them\",\"their\",\"we\",\"our\",\"you\",\"your\",\n",
        "    \"i\",\"me\",\"my\",\"he\",\"him\",\"his\",\"she\",\"her\",\n",
        "    \"not\",\"no\",\"do\",\"does\",\"did\",\"doing\"\n",
        "}\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    \"\"\"\n",
        "    Returns list of terms for indexing:\n",
        "    - lowercase\n",
        "    - tokenize\n",
        "    - keep alphabetic tokens only\n",
        "    - remove custom stopwords\n",
        "    - apply stemming\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    terms = []\n",
        "    for tok in tokens:\n",
        "        if tok.isalpha() and tok not in CUSTOM_STOPWORDS:\n",
        "            terms.append(stemmer.stem(tok))\n",
        "    return terms\n",
        "\n",
        "# --- Embedding model (for semantic retrieval) ---\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# --- FAISS index (stores embeddings for doc-level retrieval) ---\n",
        "faiss_index = None\n",
        "vector_dim = None\n",
        "\n",
        "# Parallel stores (FAISS row -> doc data)\n",
        "vector_doc_ids = []   # doc_id\n",
        "vector_texts = []     # full doc text\n",
        "\n",
        "print(\" Core setup ready (custom stopwords + stemming + embeddings + FAISS)\")"
      ],
      "metadata": {
        "id": "eOcug8jcPQ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Wikipedia source links (seed documents for the corpus)\n",
        "\n",
        "wiki_links = [\n",
        "    \"https://en.wikipedia.org/wiki/Plant_disease\",\n",
        "    \"https://en.wikipedia.org/wiki/Plant_pathology\",\n",
        "    \"https://en.wikipedia.org/wiki/Fungus\",\n",
        "    \"https://en.wikipedia.org/wiki/Bacterial_wilt\",\n",
        "    \"https://en.wikipedia.org/wiki/Powdery_mildew\"\n",
        "]\n",
        "\n",
        "print(\"Wikipedia links used:\")\n",
        "for i, link in enumerate(wiki_links, 1):\n",
        "    print(f\"{i}. {link}\")\n"
      ],
      "metadata": {
        "id": "F5Swqy3GdK37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Load documents from Wikipedia (API fetch + normalization + metadata)\n",
        "\n",
        "import requests\n",
        "import re\n",
        "\n",
        "WIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "# Wikipedia blocks requests without a proper User-Agent sometimes\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"HW02-Cloud-RAG/1.0 (student project; contact: student@example.com)\"\n",
        "}\n",
        "\n",
        "def title_from_wiki_url(url: str) -> str:\n",
        "    if \"/wiki/\" not in url:\n",
        "        raise ValueError(f\"Unsupported Wikipedia URL: {url}\")\n",
        "    title = url.split(\"/wiki/\", 1)[1]\n",
        "    title = title.split(\"#\", 1)[0]      # remove anchors\n",
        "    title = title.replace(\"_\", \" \")\n",
        "    return title\n",
        "\n",
        "def fetch_page_extract_by_title(title: str):\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"prop\": \"extracts|info\",\n",
        "        \"titles\": title,\n",
        "        \"inprop\": \"url\",\n",
        "        \"explaintext\": True,\n",
        "        \"redirects\": 1,   # follow redirects\n",
        "        \"origin\": \"*\"     # helps in some environments\n",
        "    }\n",
        "    r = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=30)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    pages = r.json()[\"query\"][\"pages\"]\n",
        "    page = next(iter(pages.values()))\n",
        "\n",
        "    # Handle missing page\n",
        "    if \"missing\" in page:\n",
        "        return {\"pageid\": None, \"title\": title, \"url\": \"\", \"text\": \"\"}\n",
        "\n",
        "    return {\n",
        "        \"pageid\": page.get(\"pageid\"),\n",
        "        \"title\": page.get(\"title\", title),\n",
        "        \"url\": page.get(\"fullurl\", \"\"),\n",
        "        \"text\": page.get(\"extract\", \"\")\n",
        "    }\n",
        "\n",
        "def slugify(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s)\n",
        "    return s.strip(\"-\")\n",
        "\n",
        "def load_docs_from_wiki_links(wiki_links):\n",
        "    docs = {}\n",
        "    docs_meta = {}\n",
        "\n",
        "    for url in wiki_links:\n",
        "        title = title_from_wiki_url(url)\n",
        "        data = fetch_page_extract_by_title(title)\n",
        "\n",
        "        text = (data.get(\"text\") or \"\").strip()\n",
        "        if not text:\n",
        "            print(f\"Empty/blocked page: {title} | {url}\")\n",
        "            continue\n",
        "\n",
        "        doc_id = f\"wiki_{slugify(data['title'])}\"\n",
        "        docs[doc_id] = text\n",
        "        docs_meta[doc_id] = {\n",
        "            \"title\": data[\"title\"],\n",
        "            \"url\": data.get(\"url\") or url,\n",
        "            \"source\": \"wikipedia\",\n",
        "            \"pageid\": data.get(\"pageid\"),\n",
        "        }\n",
        "\n",
        "        print(f\"Loaded: {data['title']} -> {doc_id} | chars={len(text)}\")\n",
        "\n",
        "    return docs, docs_meta\n",
        "\n",
        "docs, docs_meta = load_docs_from_wiki_links(wiki_links)\n",
        "print(\"Docs loaded:\", len(docs))\n"
      ],
      "metadata": {
        "id": "ka2aEOAOgS2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Build the required index (term -> DocIDs) + build FAISS embeddings store (doc-level)\n",
        "\n",
        "# 1) Build inverted index (term -> DocIDs)\n",
        "inv_index = InvertedIndexStore()\n",
        "\n",
        "for doc_id, text in docs.items():\n",
        "    terms = preprocess_text(text)   # uses custom stopwords + stemming\n",
        "    for t in set(terms):            # presence only (not frequency)\n",
        "        inv_index.add_occurrence(t, doc_id)\n",
        "\n",
        "print(f\" Inverted index built. Unique terms: {inv_index.count_terms()}\")\n",
        "\n",
        "# 2) Build embeddings + FAISS (one vector per doc)\n",
        "doc_ids = list(docs.keys())\n",
        "texts = [docs[d] for d in doc_ids]\n",
        "\n",
        "emb = embed_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "\n",
        "vector_dim = emb.shape[1]\n",
        "faiss_index = faiss.IndexFlatIP(vector_dim)  # cosine similarity via normalized embeddings\n",
        "faiss_index.add(emb)\n",
        "\n",
        "# parallel arrays for retrieval results\n",
        "vector_doc_ids = doc_ids\n",
        "vector_texts = texts\n",
        "\n",
        "print(f\" FAISS built. Vectors: {faiss_index.ntotal} | dim={vector_dim}\")\n"
      ],
      "metadata": {
        "id": "x1SNUhlfQPfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Firebase / Firestore initialization (cloud persistence setup)\n",
        "\n",
        "!pip -q install firebase-admin\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "from google.colab import userdata\n",
        "import json\n",
        "\n",
        "# 1. Retrieve the JSON content from Colab Secrets\n",
        "try:\n",
        "    key_content = userdata.get('FIREBASE_KEY')\n",
        "    # 2. Convert the string to a JSON dictionary\n",
        "    key_dict = json.loads(key_content)\n",
        "\n",
        "    # 3. Initialize Firebase Admin SDK using the dictionary (NOT a file path)\n",
        "    cred = credentials.Certificate(key_dict)\n",
        "\n",
        "    # Prevent re-initialization errors in notebook environments\n",
        "    if not firebase_admin._apps:\n",
        "        firebase_admin.initialize_app(cred)\n",
        "\n",
        "    # Create Firestore client\n",
        "    db = firestore.client()\n",
        "    print(\"Firestore connected:\", db.project)\n",
        "\n",
        "except  Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Make sure you added the secret 'FIREBASE_KEY' in the Colab sidebar (Key icon ðŸ”‘) and enabled 'Notebook access'.\")"
      ],
      "metadata": {
        "id": "z3PfU1boz_l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Upload inverted index to Firestore (cloud storage of term â†’ DocIDs)\n",
        "\n",
        "from google.cloud.firestore_v1 import ArrayUnion\n",
        "\n",
        "def upload_inverted_index(inv_index, collection_name=\"inverted_index\", batch_size=400):\n",
        "    \"\"\"\n",
        "    Uploads the inverted index to Firestore.\n",
        "\n",
        "    Each term is stored as a document in the collection:\n",
        "      inverted_index/{term}\n",
        "\n",
        "    Stored fields:\n",
        "      - term     : the stemmed term\n",
        "      - doc_ids  : list of document IDs containing the term\n",
        "      - df       : document frequency (number of documents containing the term)\n",
        "\n",
        "    Batch writes are used to stay within Firestore limits and improve performance.\n",
        "    \"\"\"\n",
        "    col = db.collection(collection_name)\n",
        "    records = inv_index.to_required_format()  # [{\"term\": ..., \"DocIDs\": [...]}, ...]\n",
        "\n",
        "    batch = db.batch()\n",
        "    ops = 0\n",
        "\n",
        "    for r in records:\n",
        "        term = r[\"term\"]\n",
        "        doc_ids = r[\"DocIDs\"]\n",
        "\n",
        "        # Use the term itself as the Firestore document ID (trimmed for safety)\n",
        "        doc_id = term[:1500]\n",
        "\n",
        "        ref = col.document(doc_id)\n",
        "        batch.set(ref, {\n",
        "            \"term\": term,\n",
        "            \"doc_ids\": doc_ids,\n",
        "            \"df\": len(doc_ids),\n",
        "        })\n",
        "\n",
        "        ops += 1\n",
        "        if ops >= batch_size:\n",
        "            batch.commit()\n",
        "            batch = db.batch()\n",
        "            ops = 0\n",
        "\n",
        "    # Commit any remaining operations\n",
        "    if ops > 0:\n",
        "        batch.commit()\n",
        "\n",
        "    print(f\"Uploaded {len(records)} terms to Firestore collection '{collection_name}'\")\n",
        "\n",
        "# Upload the built inverted index\n",
        "upload_inverted_index(inv_index)\n"
      ],
      "metadata": {
        "id": "3GMKBFBw1sdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Upload Wikipedia document metadata to Firestore (documents collection)\n",
        "\n",
        "def upload_wiki_meta(docs_meta, collection_name=\"documents\", batch_size=400):\n",
        "    \"\"\"\n",
        "    Uploads Wikipedia document metadata to Firestore.\n",
        "\n",
        "    Each document is stored as:\n",
        "      documents/{doc_id}\n",
        "\n",
        "    Stored fields:\n",
        "      - doc_id  : your internal document ID (e.g., wiki_plant-disease)\n",
        "      - title   : Wikipedia page title\n",
        "      - url     : Wikipedia page URL\n",
        "      - source  : \"wikipedia\"\n",
        "      - pageid  : Wikipedia page id (if available)\n",
        "\n",
        "    This does NOT upload the full article text; it only uploads metadata.\n",
        "    \"\"\"\n",
        "    col = db.collection(collection_name)\n",
        "\n",
        "    batch = db.batch()\n",
        "    ops = 0\n",
        "\n",
        "    for doc_id, meta in docs_meta.items():\n",
        "        ref = col.document(doc_id)\n",
        "        batch.set(ref, {\n",
        "            \"doc_id\": doc_id,\n",
        "            \"title\": meta.get(\"title\", \"\"),\n",
        "            \"url\": meta.get(\"url\", \"\"),\n",
        "            \"source\": meta.get(\"source\", \"wikipedia\"),\n",
        "            \"pageid\": meta.get(\"pageid\", None),\n",
        "        }, merge=True)\n",
        "\n",
        "        ops += 1\n",
        "        if ops >= batch_size:\n",
        "            batch.commit()\n",
        "            batch = db.batch()\n",
        "            ops = 0\n",
        "\n",
        "    if ops > 0:\n",
        "        batch.commit()\n",
        "\n",
        "    print(f\"Uploaded {len(docs_meta)} wiki docs to '{collection_name}'\")\n",
        "\n",
        "# Upload metadata for the loaded Wikipedia docs\n",
        "upload_wiki_meta(docs_meta)\n"
      ],
      "metadata": {
        "id": "Al003f-n5Ubq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Embedding-based document retrieval using FAISS (semantic search)\n",
        "\n",
        "def retrieve_top_docs(query: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Retrieves the top-K most relevant documents for a user query using\n",
        "    vector embeddings and FAISS similarity search.\n",
        "\n",
        "    This function:\n",
        "      1) Embeds the query using the same embedding model as the documents\n",
        "      2) Searches the FAISS index using cosine similarity\n",
        "      3) Returns ranked documents with titles, similarity scores, and text snippets\n",
        "\n",
        "    Note: This is retrieval only (no generation / no LLM).\n",
        "    \"\"\"\n",
        "    if faiss_index is None or faiss_index.ntotal == 0:\n",
        "        return \"FAISS index is empty. Build vectors first.\"\n",
        "\n",
        "    # Embed and normalize the query\n",
        "    q_emb = embed_model.encode(\n",
        "        [query],\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    # Search FAISS index\n",
        "    distances, indices = faiss_index.search(q_emb, top_k)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"Query: {query}\")\n",
        "    lines.append(\"=\" * 60)\n",
        "\n",
        "    # Format ranked results\n",
        "    for rank, idx in enumerate(indices[0], start=1):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "\n",
        "        doc_id = vector_doc_ids[idx]\n",
        "        title = docs_meta.get(doc_id, {}).get(\"title\", \"\")\n",
        "        text = vector_texts[idx]\n",
        "        snippet = re.sub(r\"\\s+\", \" \", text)[:350]\n",
        "        score = float(distances[0][rank - 1])\n",
        "\n",
        "        lines.append(f\"{rank}) {doc_id} | {title} | similarity: {score:.4f}\")\n",
        "        lines.append(f\"Snippet: {snippet}...\")\n",
        "        lines.append(\"-\" * 60)\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "print(\"Retrieval function ready\")"
      ],
      "metadata": {
        "id": "dIvaQbcITaV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: RAG-style output (retrieval + \"enriched\" answer without OpenAI)\n",
        "# We will: retrieve top docs, then produce a simple enriched response by extracting key sentences.\n",
        "\n",
        "def split_sentences(text: str):\n",
        "    # simple sentence split (good enough for baseline)\n",
        "    parts = re.split(r'(?<=[.!?])\\s+', re.sub(r\"\\s+\", \" \", text).strip())\n",
        "    return [s for s in parts if len(s) > 30]\n",
        "\n",
        "def rag_answer_without_llm(query: str, top_k: int = 3, max_sentences_per_doc: int = 2):\n",
        "    if faiss_index is None or faiss_index.ntotal == 0:\n",
        "        return \"FAISS index is empty. Build vectors first.\"\n",
        "\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "    distances, indices = faiss_index.search(q_emb, top_k)\n",
        "\n",
        "    lines = []\n",
        "    lines.append(f\"Query: {query}\")\n",
        "    lines.append(\"=\" * 60)\n",
        "\n",
        "    # Retrieval section\n",
        "    lines.append(\"Top retrieved documents:\")\n",
        "    retrieved = []\n",
        "    for rank, idx in enumerate(indices[0], start=1):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        doc_id = vector_doc_ids[idx]\n",
        "        title = docs_meta.get(doc_id, {}).get(\"title\", \"\")\n",
        "        score = float(distances[0][rank - 1])\n",
        "        retrieved.append((doc_id, title, score))\n",
        "        lines.append(f\"{rank}) {doc_id} | {title} | similarity: {score:.4f}\")\n",
        "    lines.append(\"=\" * 60)\n",
        "\n",
        "    # Enriched response (extractive, no LLM)\n",
        "    lines.append(\"Enriched response (extractive, no LLM):\")\n",
        "    q_terms = set(preprocess_text(query))\n",
        "\n",
        "    for doc_id, title, score in retrieved:\n",
        "        text = docs[doc_id]\n",
        "        sents = split_sentences(text)\n",
        "\n",
        "        # score sentences by overlap with query terms (stems)\n",
        "        scored = []\n",
        "        for s in sents:\n",
        "            s_terms = set(preprocess_text(s))\n",
        "            overlap = len(q_terms & s_terms)\n",
        "            if overlap > 0:\n",
        "                scored.append((overlap, s))\n",
        "\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "        best = [s for _, s in scored[:max_sentences_per_doc]]\n",
        "\n",
        "        lines.append(f\"- Source: {doc_id} | {title}\")\n",
        "        if best:\n",
        "            for b in best:\n",
        "                lines.append(f\"  â€¢ {b}\")\n",
        "        else:\n",
        "            lines.append(\"  â€¢ (No strong matching sentences found)\")\n",
        "        lines.append(\"-\" * 60)\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "print(\" RAG-style (no OpenAI) function ready\")\n"
      ],
      "metadata": {
        "id": "yNqRTEkVPTuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Quick demo (edit the query text)\n",
        "\n",
        "print(retrieve_top_docs(\"how to detect plant diseases using sensors and ai\", top_k=3))\n",
        "print()\n",
        "print(rag_answer_without_llm(\"how to detect plant diseases using sensors and ai\", top_k=3))\n"
      ],
      "metadata": {
        "id": "UXh_buFURvu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FROM NOW ON. ASAAD'S PART"
      ],
      "metadata": {
        "id": "ZJL2UYRadFR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install firebase-admin ipywidgets matplotlib\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "from google.colab import userdata  # Import userdata\n",
        "import json\n",
        "\n",
        "# Check if Firebase is already running to avoid re-initialization error\n",
        "if not firebase_admin._apps:\n",
        "    # Use Colab Secrets instead of a file path\n",
        "    key_content = userdata.get('FIREBASE_KEY')\n",
        "    key_dict = json.loads(key_content)\n",
        "    cred = credentials.Certificate(key_dict)\n",
        "    firebase_admin.initialize_app(cred)\n",
        "\n",
        "# Get the client (works even if initialized in previous cells)\n",
        "db = firestore.client()\n",
        "# --- FIX END ---\n",
        "\n",
        "print(\"âœ… Connected to Firestore in project:\", db.project)"
      ],
      "metadata": {
        "id": "DmIJXYU1dIqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "BASE_URL = \"https://server-cloud-v645.onrender.com\"\n",
        "\n",
        "def fetch_history(feed: str, limit: int = 30) -> pd.DataFrame:\n",
        "    \"\"\"Fetch IoT history from course server. Returns DataFrame with created_at,value.\"\"\"\n",
        "    resp = requests.get(f\"{BASE_URL}/history\", params={\"feed\": feed, \"limit\": int(limit)}, timeout=120)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    if \"data\" not in data:\n",
        "        raise ValueError(f\"Server error: {data}\")\n",
        "\n",
        "    df = pd.DataFrame(data[\"data\"])\n",
        "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
        "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"created_at\", \"value\"]).sort_values(\"created_at\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "WfyJ7ggJnUBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# -------------------------\n",
        "# Screen A â€” Plant Image\n",
        "# -------------------------\n",
        "plant_name = widgets.Text(placeholder=\"Plant name\")\n",
        "plant_upload = widgets.FileUpload(accept=\"image/*\", multiple=False)\n",
        "plant_save = widgets.Button(description=\"Save\", button_style=\"success\")\n",
        "plant_out = widgets.Output()\n",
        "\n",
        "def plant_show(_):\n",
        "    with plant_out:\n",
        "        clear_output()\n",
        "        if plant_upload.value:\n",
        "            _, f = list(plant_upload.value.items())[0]\n",
        "            display(widgets.Image(value=f[\"content\"]))\n",
        "\n",
        "def plant_save_meta(_):\n",
        "    with plant_out:\n",
        "        if not plant_upload.value or not plant_name.value.strip():\n",
        "            print(\"Upload image + plant name\")\n",
        "            return\n",
        "        filename, _f = list(plant_upload.value.items())[0]\n",
        "        db.collection(\"plant_images\").add({\n",
        "            \"plant\": plant_name.value.strip(),\n",
        "            \"file\": filename,\n",
        "            \"time\": datetime.now(timezone.utc)\n",
        "        })\n",
        "        print(\"Saved\")\n",
        "\n",
        "plant_upload.observe(plant_show, names=\"value\")\n",
        "plant_save.on_click(plant_save_meta)\n",
        "\n",
        "screenA = widgets.VBox([plant_name, plant_upload, plant_save, plant_out])\n",
        "\n",
        "# -------------------------\n",
        "# Screen B â€” IoT Data\n",
        "# -------------------------\n",
        "iot_feed = widgets.Dropdown(options=[\"humidity\",\"soil\",\"temperature\"], value=\"humidity\", description=\"Feed:\")\n",
        "iot_limit = widgets.IntSlider(value=10, min=1, max=100, step=1, description=\"Samples:\")\n",
        "iot_btn = widgets.Button(description=\"Get Data\", button_style=\"success\")\n",
        "iot_out = widgets.Output()\n",
        "\n",
        "def iot_click(_):\n",
        "    with iot_out:\n",
        "        clear_output()\n",
        "        print(\"Waking server (first request can take time)...\")\n",
        "        df = fetch_history(iot_feed.value, iot_limit.value)\n",
        "        print(\"Rows returned:\", len(df))\n",
        "        display(df)\n",
        "        print(\"\\nLatest value:\", df[\"value\"].iloc[-1], \"| at:\", df[\"created_at\"].iloc[-1])\n",
        "\n",
        "iot_btn.on_click(iot_click)\n",
        "\n",
        "screenB = widgets.VBox([iot_feed, iot_limit, iot_btn, iot_out])\n",
        "\n",
        "# -------------------------\n",
        "# Screen C â€” Query / Search\n",
        "# -------------------------\n",
        "index_box = widgets.Text(value=\"inverted_index\", description=\"Index:\")\n",
        "query_box = widgets.Text(value=\"about\", description=\"Query:\")\n",
        "search_btn = widgets.Button(description=\"Search\", button_style=\"primary\")\n",
        "search_out = widgets.Output()\n",
        "\n",
        "def search_inverted_index(index_name: str, term: str):\n",
        "    index_name = index_name.strip()\n",
        "    term = term.strip().lower()\n",
        "    if not index_name or not term:\n",
        "        return None, \"Enter both Index and Query.\"\n",
        "\n",
        "    # doc id == term\n",
        "    doc = db.collection(index_name).document(term).get()\n",
        "    if doc.exists:\n",
        "        data = doc.to_dict() or {}\n",
        "        return {\n",
        "            \"term\": term,\n",
        "            \"df\": data.get(\"df\"),\n",
        "            \"doc_ids\": data.get(\"doc_ids\", [])\n",
        "        }, None\n",
        "\n",
        "    # fallback by field\n",
        "    qs = list(db.collection(index_name).where(\"term\", \"==\", term).limit(1).stream())\n",
        "    if qs:\n",
        "        data = qs[0].to_dict() or {}\n",
        "        return {\n",
        "            \"term\": term,\n",
        "            \"df\": data.get(\"df\"),\n",
        "            \"doc_ids\": data.get(\"doc_ids\", [])\n",
        "        }, None\n",
        "\n",
        "    return None, f\"No results for '{term}' in '{index_name}'.\"\n",
        "\n",
        "def on_search(_):\n",
        "    with search_out:\n",
        "        clear_output()\n",
        "        result, err = search_inverted_index(index_box.value, query_box.value)\n",
        "        if err:\n",
        "            print(err)\n",
        "            return\n",
        "        print(\"term:\", result[\"term\"])\n",
        "        if result[\"df\"] is not None:\n",
        "            print(\"df:\", result[\"df\"])\n",
        "        print(\"doc_ids:\")\n",
        "        for x in result[\"doc_ids\"]:\n",
        "            print(\" -\", x)\n",
        "\n",
        "search_btn.on_click(on_search)\n",
        "\n",
        "screenC = widgets.VBox([index_box, query_box, search_btn, search_out])\n",
        "\n",
        "# -------------------------\n",
        "# Screen D â€” Dashboard\n",
        "# -------------------------\n",
        "dash_feed = widgets.Dropdown(options=[\"humidity\",\"soil\",\"temperature\"], value=\"soil\", description=\"Feed:\")\n",
        "dash_limit = widgets.IntSlider(value=30, min=10, max=200, step=10, description=\"Samples:\")\n",
        "dash_btn = widgets.Button(description=\"Build\", button_style=\"warning\")\n",
        "dash_out = widgets.Output()\n",
        "\n",
        "def dashboard_status(feed, latest):\n",
        "    if feed == \"soil\":\n",
        "        if latest < 30: return \"Critical\"\n",
        "        if latest < 45: return \"Warning\"\n",
        "        return \"Healthy\"\n",
        "    if feed == \"humidity\":\n",
        "        if latest < 30: return \"Warning\"\n",
        "        return \"OK\"\n",
        "    if feed == \"temperature\":\n",
        "        if latest < 10 or latest > 35: return \"Warning\"\n",
        "        return \"OK\"\n",
        "    return \"OK\"\n",
        "\n",
        "def on_dash(_):\n",
        "    with dash_out:\n",
        "        clear_output()\n",
        "        df = fetch_history(dash_feed.value, dash_limit.value)\n",
        "        latest = df[\"value\"].iloc[-1]\n",
        "        print(\"Status:\", dashboard_status(dash_feed.value, latest))\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(df[\"created_at\"], df[\"value\"], marker=\"o\")\n",
        "        plt.title(f\"{dash_feed.value} over time\")\n",
        "        plt.xlabel(\"time\")\n",
        "        plt.ylabel(\"value\")\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.show()\n",
        "\n",
        "dash_btn.on_click(on_dash)\n",
        "\n",
        "screenD = widgets.VBox([dash_feed, dash_limit, dash_btn, dash_out])\n",
        "\n",
        "# -------------------------\n",
        "# Tabs\n",
        "# -------------------------\n",
        "tabs = widgets.Tab(children=[screenA, screenB, screenC, screenD])\n",
        "tabs.set_title(0, \"Plant Image\")\n",
        "tabs.set_title(1, \"IoT Data\")\n",
        "tabs.set_title(2, \"Query/Search\")\n",
        "tabs.set_title(3, \"Dashboard\")\n",
        "\n",
        "display(tabs)\n"
      ],
      "metadata": {
        "id": "a0pCzwDm0w3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}